# fb_groups_scraper_focused.py - Focus on larger height element

import time, random, threading, re, requests, pandas as pd
from datetime import datetime
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, NoSuchElementException, StaleElementReferenceException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ----------------------------
# Helper utils
# ----------------------------

def parse_cookies_to_list(cookie_str):
    cookies_list = []
    for pair in cookie_str.split(';'):
        pair = pair.strip()
        if '=' in pair:
            name, value = pair.split('=', 1)
            cookies_list.append({'name': name.strip(), 'value': value.strip(), 'domain': '.facebook.com'})
    return cookies_list

def parse_cookies_to_dict(cookie_str):
    d = {}
    for pair in cookie_str.split(';'):
        pair = pair.strip()
        if '=' in pair:
            name, value = pair.split('=', 1)
            d[name.strip()] = value.strip()
    return d

def clean_text(text):
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text.strip())
    ui_patterns = [
        r'\b(Like|Reply|Share|Comment|Translate|Hide|Report|Block)\b',
        r'\b(Th√≠ch|Tr·∫£ l·ªùi|Chia s·∫ª|B√¨nh lu·∫≠n|D·ªãch|·∫®n|B√°o c√°o|Ch·∫∑n)\b',
        r'\b\d+\s*(min|minutes?|hours?|days?|seconds?|ph√∫t|gi·ªù|ng√†y|gi√¢y)\s*(ago|tr∆∞·ªõc)?\b',
        r'\b(Top fan|Most relevant|Newest|All comments|B√¨nh lu·∫≠n h√†ng ƒë·∫ßu)\b'
    ]
    for pattern in ui_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    return text.strip()

def is_anonymous_user(username):
    """
    Ki·ªÉm tra xem c√≥ ph·∫£i l√† ng∆∞·ªùi d√πng ·∫©n danh kh√¥ng
    Args:
        username (str): T√™n ng∆∞·ªùi d√πng c·∫ßn ki·ªÉm tra
    Returns:
        bool: True n·∫øu l√† ·∫©n danh, False n·∫øu kh√¥ng
    """
    if not username or username == "Unknown":
        return True
    
    username_lower = username.lower().strip()
    
    # C√°c pattern ƒë·ªÉ nh·∫≠n di·ªán ng∆∞·ªùi d√πng ·∫©n danh
    anonymous_patterns = [
        # Ti·∫øng Vi·ªát
        r'\b(·∫©n\s*danh|an\s*danh|ng∆∞·ªùi\s*d√πng\s*·∫©n\s*danh)\b',
        r'\b(th√†nh\s*vi√™n\s*·∫©n\s*danh|tv\s*·∫©n\s*danh)\b',
        r'\b(ng∆∞·ªùi\s*tham\s*gia\s*·∫©n\s*danh|participant\s*·∫©n\s*danh)\b',
        
        # Ti·∫øng Anh  
        r'\b(anonymous|anon)\b',
        r'\b(anonymous\s*(user|member|participant))\b',
        r'\b(hidden\s*(user|member|participant))\b',
        r'\b(private\s*(user|member|participant))\b',
        
        # C√°c pattern ph·ªï bi·∫øn kh√°c
        r'\b(user\s*\d+)\b',  # user123, user456
        r'\b(member\s*\d+)\b',  # member123
        r'\b(participant\s*\d+)\b',  # participant123
        r'\b(guest\s*\d*)\b',  # guest, guest123
        r'\b(unknown\s*(user|member))\b',
        r'^\d+$',  # Ch·ªâ l√† s·ªë
        r'^[a-f0-9]{8,}$',  # Hash string d√†i
        
        # Facebook specific
        r'\b(facebook\s*user)\b',
        r'\b(fb\s*user)\b',
        r'\b(deleted\s*(user|account))\b',
        r'\b(deactivated\s*(user|account))\b',
    ]
    
    # Ki·ªÉm tra t·ª´ng pattern
    for pattern in anonymous_patterns:
        if re.search(pattern, username_lower):
            print(f"    üö´ Detected anonymous user: '{username}' (matched: {pattern})")
            return True
    
    # Ki·ªÉm tra c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
    # Username qu√° ng·∫Øn (< 2 k√Ω t·ª±)
    if len(username.strip()) < 2:
        print(f"    üö´ Username too short: '{username}'")
        return True
    
    # Username ch·ªâ ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát
    if re.match(r'^[^\w\s]+$', username):
        print(f"    üö´ Username only special chars: '{username}'")
        return True
    
    # Username c√≥ pattern nghi ng·ªù (nhi·ªÅu s·ªë li√™n ti·∫øp)
    if re.search(r'\d{6,}', username):
        print(f"    üö´ Username has suspicious number pattern: '{username}'")
        return True
    
    print(f"    ‚úÖ Valid username: '{username}'")
    return False

def safe_get_element_text(element, max_retries=3):
    """
    Safely get text from element with retry mechanism for stale elements
    Args:
        element: Selenium WebElement
        max_retries (int): Maximum number of retries
    Returns:
        str: Element text or empty string if failed
    """
    for attempt in range(max_retries):
        try:
            return element.text.strip()
        except StaleElementReferenceException:
            print(f"    ‚ö†Ô∏è Stale element on attempt {attempt + 1}, retrying...")
            time.sleep(0.5)
            continue
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error getting element text: {e}")
            return ""
    
    print(f"    ‚ùå Failed to get element text after {max_retries} attempts")
    return ""

def safe_get_element_attribute(element, attribute, max_retries=3):
    """
    Safely get attribute from element with retry mechanism for stale elements
    Args:
        element: Selenium WebElement
        attribute (str): Attribute name
        max_retries (int): Maximum number of retries
    Returns:
        str: Attribute value or empty string if failed
    """
    for attempt in range(max_retries):
        try:
            return element.get_attribute(attribute) or ""
        except StaleElementReferenceException:
            print(f"    ‚ö†Ô∏è Stale element on attempt {attempt + 1}, retrying...")
            time.sleep(0.5)
            continue
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error getting element attribute: {e}")
            return ""
    
    print(f"    ‚ùå Failed to get element attribute after {max_retries} attempts")
    return ""

def safe_find_elements(element, by, value, max_retries=3):
    """
    Safely find elements with retry mechanism for stale elements
    Args:
        element: Selenium WebElement
        by: Locator strategy
        value: Locator value
        max_retries (int): Maximum number of retries
    Returns:
        list: List of WebElements or empty list if failed
    """
    for attempt in range(max_retries):
        try:
            return element.find_elements(by, value)
        except StaleElementReferenceException:
            print(f"    ‚ö†Ô∏è Stale element on attempt {attempt + 1}, retrying...")
            time.sleep(0.5)
            continue
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error finding elements: {e}")
            return []
    
    print(f"    ‚ùå Failed to find elements after {max_retries} attempts")
    return []

def extract_uid_from_profile_url(profile_url):
    """
    Extract UID t·ª´ Facebook profile URL
    Args:
        profile_url (str): URL profile Facebook
    Returns:
        str: UID ho·∫∑c "Unknown"
    """
    if not profile_url:
        return "Unknown"
    
    try:
        # C√°c pattern ƒë·ªÉ extract UID t·ª´ URL
        uid_patterns = [
            r'profile\.php\?id=(\d+)',
            r'user\.php\?id=(\d+)', 
            r'/user/(\d+)',
            r'id=(\d+)',
            r'facebook\.com/profile\.php\?id=(\d+)',
            r'facebook\.com/(\d{10,})',  # Direct UID in URL
            r'(\d{10,})'  # Facebook UIDs th∆∞·ªùng c√≥ 10+ ch·ªØ s·ªë
        ]
        
        for pattern in uid_patterns:
            match = re.search(pattern, profile_url)
            if match:
                uid = match.group(1)
                if len(uid) >= 10:  # Validate UID length
                    print(f"    ‚úÖ Extracted UID from URL: {uid}")
                    return uid
        
        # N·∫øu URL c√≥ d·∫°ng facebook.com/username, th·ª≠ extract username
        username_match = re.search(r'facebook\.com/([^/?]+)', profile_url)
        if username_match:
            username = username_match.group(1)
            if not username.isdigit() and len(username) > 2:
                print(f"    üîÑ Found username in URL: {username}, will try to resolve to UID")
                return f"username:{username}"  # ƒê√°nh d·∫•u ƒë·ªÉ x·ª≠ l√Ω sau
        
        return "Unknown"
        
    except Exception as e:
        print(f"    ‚ö†Ô∏è Error extracting UID from URL: {e}")
        return "Unknown"

def get_uid_from_username(username, cookies_dict=None, driver=None):
    """
    OPTIMIZED: L·∫•y UID Facebook t·ª´ username v·ªõi performance improvements
    Args:
        username (str): Username Facebook
        cookies_dict (dict): Dictionary cookies ƒë·ªÉ authenticate  
        driver: Selenium WebDriver instance (optional)
    Returns:
        str: UID Facebook ho·∫∑c "Unknown" n·∫øu kh√¥ng t√¨m th·∫•y
    """
    if not username or username == "Unknown":
        return "Unknown"
    
    try:
        # Chu·∫©n h√≥a username
        clean_username = username.strip()
        if clean_username.startswith('https://'):
            if 'facebook.com/' in clean_username:
                clean_username = clean_username.split('facebook.com/')[-1].split('?')[0].split('/')[0]
        
        print(f"  üîç OPTIMIZED UID resolution for: {clean_username}")
        
        # OPTIMIZED Method 1: S·ª≠ d·ª•ng Selenium (nh∆∞ng nhanh h∆°n)
        if driver:
            try:
                print(f"    ‚ö° Fast Selenium resolve...")
                
                profile_url = f"https://www.facebook.com/{clean_username}"
                current_url = driver.current_url
                
                # Navigate to profile v·ªõi timeout ng·∫Øn h∆°n
                driver.get(profile_url)
                time.sleep(1.5)  # Gi·∫£m t·ª´ 3s xu·ªëng 1.5s
                
                final_url = driver.current_url
                print(f"    üìç Final URL: {final_url[:80]}...")
                
                # Extract UID from final URL
                uid_match = re.search(r'profile\.php\?id=(\d+)', final_url)
                if uid_match:
                    uid = uid_match.group(1)
                    print(f"    ‚úÖ Fast UID via URL: {uid}")
                    
                    # Quick restore
                    driver.get(current_url)
                    time.sleep(0.5)  # Gi·∫£m t·ª´ 2s xu·ªëng 0.5s
                    return uid
                
                # Quick page source scan (ch·ªâ scan patterns quan tr·ªçng nh·∫•t)
                page_source = driver.page_source
                quick_patterns = [
                    r'"entity_id":"(\d+)"',
                    r'"userID":"(\d+)"',
                    r'"profile_id":"(\d+)"'
                ]
                
                for pattern in quick_patterns:
                    matches = re.findall(pattern, page_source)
                    if matches:
                        uid = matches[0]
                        if len(uid) >= 10:
                            print(f"    ‚úÖ Fast UID via source: {uid}")
                            driver.get(current_url)
                            time.sleep(0.5)
                            return uid
                
                # Quick restore
                driver.get(current_url)
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è Fast Selenium failed: {e}")
                try:
                    driver.get(current_url)
                    time.sleep(0.5)
                except:
                    pass
        
        # Method 2: S·ª≠ d·ª•ng requests (fallback)
        print(f"    üåê Using requests to resolve UID...")
        
        # T·∫°o URL profile t·ª´ username
        profile_urls = [
            f"https://www.facebook.com/{clean_username}",
            f"https://m.facebook.com/{clean_username}",
            f"https://mbasic.facebook.com/{clean_username}"
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # Th√™m cookies n·∫øu c√≥
        if cookies_dict:
            cookie_string = '; '.join([f"{k}={v}" for k, v in cookies_dict.items()])
            headers['Cookie'] = cookie_string
        
        for url in profile_urls:
            try:
                print(f"    üîç Trying to get UID from: {url}")
                
                response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
                
                if response.status_code == 200:
                    content = response.text
                    
                    # T√¨m UID trong response
                    uid_patterns = [
                        r'"entity_id":"(\d+)"',
                        r'"userID":"(\d+)"',
                        r'"user_id":"(\d+)"',
                        r'"id":"(\d+)"',
                        r'profile\.php\?id=(\d+)',
                        r'user\.php\?id=(\d+)',
                        r'"profile_id":"(\d+)"',
                        r'entity_id=(\d+)',
                        r'profile_owner":"(\d+)"',
                        r'"pageID":"(\d+)"',
                        r'data-profileid="(\d+)"',
                        r'data-userid="(\d+)"',
                        r'"actorID":"(\d+)"',
                        r'"target_id":"(\d+)"'
                    ]
                    
                    for pattern in uid_patterns:
                        matches = re.findall(pattern, content)
                        if matches:
                            # L·∫•y UID ƒë·∫ßu ti√™n t√¨m th·∫•y (th∆∞·ªùng l√† UID ch√≠nh x√°c nh·∫•t)
                            uid = matches[0]
                            # Validate UID (Facebook UID th∆∞·ªùng c√≥ √≠t nh·∫•t 10 ch·ªØ s·ªë)
                            if len(uid) >= 10 and uid.isdigit():
                                print(f"    ‚úÖ Found UID: {uid} using pattern: {pattern}")
                                return uid
                    
                    # Fallback: t√¨m trong redirected URL
                    if 'profile.php?id=' in response.url:
                        uid_match = re.search(r'profile\.php\?id=(\d+)', response.url)
                        if uid_match:
                            uid = uid_match.group(1)
                            print(f"    ‚úÖ Found UID from redirect URL: {uid}")
                            return uid
                
            except requests.RequestException as e:
                print(f"    ‚ö†Ô∏è Request failed for {url}: {e}")
                continue
            except Exception as e:
                print(f"    ‚ö†Ô∏è Error processing {url}: {e}")
                continue
        
        print(f"    ‚ùå Could not find UID for username: {username}")
        return "Unknown"
        
    except Exception as e:
        print(f"‚ùå Error in get_uid_from_username: {e}")
        return "Unknown"

# ----------------------------
# FOCUSED Facebook Groups Scraper
# ----------------------------

class FacebookGroupsScraper:
    def __init__(self, cookie_str, headless=True):
        options = Options()
        if headless:
            options.add_argument("--headless=new")
        options.add_argument("--disable-notifications")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")

        # Better user agent for modern Facebook
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
        options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        self.wait = WebDriverWait(self.driver, 15)
        self.cookie_str = cookie_str or ""
        self.cookies_list = parse_cookies_to_list(self.cookie_str)
        self.cookies_dict = parse_cookies_to_dict(self.cookie_str)
        self._stop_flag = False
        self.current_layout = None
        self._anonymous_filtered_count = 0
        
        if self.cookies_list:
            self._login_with_cookies()

    def _login_with_cookies(self):
        # Start with regular Facebook for better groups access
        self.driver.get("https://www.facebook.com")
        time.sleep(3)
        
        for c in self.cookies_list:
            cookie = c.copy()
            cookie.pop('sameSite', None)
            cookie.pop('httpOnly', None) 
            cookie.pop('secure', None)
            cookie.setdefault('domain', '.facebook.com')
            try:
                self.driver.add_cookie(cookie)
            except: 
                pass
        
        self.driver.get("https://www.facebook.com")
        time.sleep(4)

    def load_post(self, post_url):
        print(f"Loading groups post: {post_url}")
        
        urls_to_try = []
        
        if "groups/" in post_url:
            # Try www first for groups, then mobile, then mbasic
            www_url = post_url.replace("mbasic.facebook.com", "www.facebook.com").replace("m.facebook.com", "www.facebook.com")
            mobile_url = post_url.replace("www.facebook.com", "m.facebook.com").replace("mbasic.facebook.com", "m.facebook.com")
            mbasic_url = post_url.replace("www.facebook.com", "mbasic.facebook.com").replace("m.facebook.com", "mbasic.facebook.com")
            
            urls_to_try = [www_url, mobile_url, mbasic_url]
        else:
            urls_to_try = [post_url]
        
        for url_attempt in urls_to_try:
            try:
                print(f"Trying URL: {url_attempt}")
                self.driver.get(url_attempt)
                time.sleep(6)
                
                current_url = self.driver.current_url
                page_title = self.driver.title
                
                print(f"Current URL: {current_url}")
                print(f"Page title: {page_title}")
                
                # Detect layout
                if "m.facebook.com" in current_url:
                    self.current_layout = "mobile"
                elif "mbasic.facebook.com" in current_url:
                    self.current_layout = "mbasic"
                else:
                    self.current_layout = "www"
                
                print(f"Detected layout: {self.current_layout}")
                
                # Check login status
                if any(keyword in page_title.lower() for keyword in ["log in", "login", "ƒëƒÉng nh·∫≠p"]):
                    print("‚ùå Not logged in with this URL, trying next...")
                    continue
                
                print(f"‚úÖ Successfully loaded groups post with {self.current_layout} layout")
                
                # Try to switch to "All comments" view
                self._switch_to_all_comments()

                # Try to click "View more comments" button
                self._click_view_more()
                
                return True
                    
            except Exception as e:
                print(f"Failed to load {url_attempt}: {e}")
                continue
        
        print("‚ùå Failed to load post with any URL variant")
        return False

    def clear_page_cache(self):
        """Clear page cache and force reload to ensure fresh DOM"""
        try:
            print("üßπ Clearing page cache...")
            
            # Clear browser cache
            self.driver.execute_script("window.localStorage.clear();")
            self.driver.execute_script("window.sessionStorage.clear();")
            
            # Force page refresh
            self.driver.refresh()
            time.sleep(5)  # Wait for fresh load
            
            print("‚úÖ Page cache cleared and refreshed")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error clearing cache: {e}")

    def _switch_to_all_comments(self):
        """Switch to 'All comments' view to get more comments"""
        print("üîÑ Attempting to switch to 'All comments' view...")
        
        try:
            time.sleep(3)
            
            # Enhanced selectors for all comments button
            all_comments_selectors = [
                # Vietnamese selectors
                "//span[contains(text(),'T·∫•t c·∫£ b√¨nh lu·∫≠n')]",
                "//div[contains(text(),'T·∫•t c·∫£ b√¨nh lu·∫≠n')]",
                "//a[contains(text(),'T·∫•t c·∫£ b√¨nh lu·∫≠n')]",
                "//button[contains(text(),'T·∫•t c·∫£ b√¨nh lu·∫≠n')]",
                
                # English selectors
                "//span[contains(text(),'All comments')]",
                "//div[contains(text(),'All comments')]",
                "//a[contains(text(),'All comments')]",
                "//button[contains(text(),'All comments')]",
                
                # Role-based selectors
                "//div[@role='button' and (contains(text(),'T·∫•t c·∫£') or contains(text(),'All'))]",
                "//span[@role='button' and (contains(text(),'T·∫•t c·∫£') or contains(text(),'All'))]",
                
                # Aria-label selectors
                "//div[contains(@aria-label,'comment') and contains(text(),'All')]",
                "//div[contains(@aria-label,'b√¨nh lu·∫≠n') and contains(text(),'T·∫•t c·∫£')]"
            ]
            
            clicked = False
            self.all_comments_button = None  # Store the button for later use
            
            for selector in all_comments_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    for element in elements:
                        if element.is_displayed() and element.is_enabled():
                            print(f"  Found 'All comments' button: {element.text}")
                            
                            # Store the button for later use
                            self.all_comments_button = element
                            
                            # Scroll into view
                            self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", element)
                            time.sleep(1)
                            
                            # Try to click
                            try:
                                element.click()
                                clicked = True
                                print("  ‚úÖ Successfully clicked 'All comments' button")
                                time.sleep(4)  # Wait for comments to load
                                break
                            except:
                                # Try JavaScript click
                                try:
                                    self.driver.execute_script("arguments[0].click();", element)
                                    clicked = True
                                    print("  ‚úÖ Successfully clicked 'All comments' button (JS)")
                                except:
                                    continue

                            # Click on div with role="menuitem" and tabindex="0"
                            try:
                                menuitem_element = self.driver.find_element(By.XPATH, "//div[@role='menuitem' and @tabindex='0']")
                                self.driver.execute_script("arguments[0].click();", menuitem_element)
                                print("  ‚úÖ Successfully clicked menuitem div")
                                time.sleep(2)  # Wait for any menu actions to complete
                            except Exception as e:
                                print(f"  ‚ö†Ô∏è Could not find or click menuitem div: {e}")
                            
                            time.sleep(4)
                            break
                    
                    if clicked:
                        break
                        
                except Exception as e:
                    continue
            
            if not clicked:
                print("  ‚ö†Ô∏è Could not find or click 'All comments' button, proceeding with current view")
            else:
                print("  üéØ Switched to 'All comments' view successfully")
                
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error switching to 'All comments' view: {e}")
            print("  Proceeding with current view...")

    def _click_view_more(self):
        """Click on 'View more comments' button to load more comments"""
        print("üîÑ Attempting to click 'View more comments' button...")
        
        try:
            time.sleep(3)
            
            # Enhanced selectors for view more button
            view_more_selectors = [
                "//div[contains(text(),'View more comments')]",
                "//button[contains(text(),'View more comments')]",
                "//a[contains(text(),'View more comments')]",
                "//span[contains(text(),'View more comments')]"
            ]

            clicked = False
            for selector in view_more_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    for element in elements:
                        if element.is_displayed() and element.is_enabled():
                            print(f"  Found 'View more comments' button: {element.text}")
                            
                            # Scroll into view
                            self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", element)
                            time.sleep(1)
                            
                            # Try to click
                            try:
                                element.click()
                                clicked = True
                                print("  ‚úÖ Successfully clicked 'View more comments' button")
                                time.sleep(4)  # Wait for comments to load
                                break
                            except:
                                # Try JavaScript click
                                try:
                                    self.driver.execute_script("arguments[0].click();", element)
                                    clicked = True
                                    print("  ‚úÖ Successfully clicked 'View more comments' button (JS)")
                                    
                                    time.sleep(4)
                                    break
                                except:
                                    continue
                    
                    if clicked:
                        break
                        
                except Exception as e:
                    continue
            
            if not clicked:
                print("  ‚ö†Ô∏è Could not find or click 'View more comments' button, proceeding with current view")
            else:
                print("  üéØ Switched to 'View more comments' view successfully")
                
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error switching to 'View more comments' view: {e}")
            print("  Proceeding with current view...")

    def refresh_stale_elements(self, elements_list):
        """
        Refresh stale elements by re-finding them
        Args:
            elements_list (list): List of potentially stale elements
        Returns:
            list: List of refreshed elements
        """
        print("üîÑ Refreshing potentially stale elements...")
        refreshed_elements = []
        
        for i, element in enumerate(elements_list):
            try:
                # Test if element is stale
                _ = element.tag_name
                refreshed_elements.append(element)
            except StaleElementReferenceException:
                print(f"  ‚ö†Ô∏è Element {i+1} is stale, attempting to refresh...")
                try:
                    # Try to re-find element by its position and attributes
                    location = element.location
                    size = element.size
                    
                    # Find elements near the same location
                    nearby_elements = self.driver.find_elements(By.XPATH, "//*")
                    
                    for candidate in nearby_elements:
                        try:
                            candidate_location = candidate.location
                            candidate_size = candidate.size
                            
                            # Check if location and size match approximately
                            if (abs(candidate_location['x'] - location['x']) < 10 and
                                abs(candidate_location['y'] - location['y']) < 10 and
                                abs(candidate_size['width'] - size['width']) < 50 and
                                abs(candidate_size['height'] - size['height']) < 50):
                                
                                refreshed_elements.append(candidate)
                                print(f"    ‚úÖ Refreshed element {i+1}")
                                break
                        except:
                            continue
                    else:
                        print(f"    ‚ùå Could not refresh element {i+1}")
                        
                except Exception as refresh_error:
                    print(f"    ‚ùå Error refreshing element {i+1}: {refresh_error}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error checking element {i+1}: {e}")
                continue
        
        print(f"‚úÖ Refreshed {len(refreshed_elements)}/{len(elements_list)} elements")
        return refreshed_elements

    def extract_with_retry(self, element, index, max_retries=2):
        """
        Extract comment data with retry mechanism for stale elements
        Args:
            element: Selenium WebElement
            index (int): Element index
            max_retries (int): Maximum retry attempts
        Returns:
            dict or None: Comment data or None if failed
        """
        for attempt in range(max_retries + 1):
            try:
                return self.extract_comment_data_focused(element, index)
            except StaleElementReferenceException:
                if attempt < max_retries:
                    print(f"    ‚ö†Ô∏è Stale element on attempt {attempt + 1}, retrying after refresh...")
                    time.sleep(1)
                    # Try to refresh page partially
                    try:
                        self.driver.execute_script("window.scrollBy(0, -100);")
                        time.sleep(0.5)
                        self.driver.execute_script("window.scrollBy(0, 100);")
                        time.sleep(0.5)
                    except:
                        pass
                    continue
                else:
                    print(f"    ‚ùå Element still stale after {max_retries} retries, skipping...")
                    return None
            except Exception as e:
                print(f"    ‚ö†Ô∏è Non-stale error on attempt {attempt + 1}: {e}")
                if attempt < max_retries:
                    time.sleep(0.5)
                    continue
                else:
                    return None
        
        return None

    def extract_fresh_comments_from_container(self, container_element):
        """
        Extract fresh comment elements from container to avoid stale references
        Args:
            container_element: Parent container element
        Returns:
            list: Fresh comment elements
        """
        print("üîÑ Extracting fresh comment elements from container...")
        fresh_elements = []
        
        try:
            # Strategy 1: Find all divs in container and filter for comments
            all_divs = container_element.find_elements(By.XPATH, ".//div")
            print(f"  Found {len(all_divs)} total divs in container")
            
            comment_count = 0
            for div in all_divs:
                try:
                    if self.is_comment_div(div):
                        fresh_elements.append(div)
                        comment_count += 1
                        
                        # Log every 10th comment for progress
                        if comment_count % 10 == 0:
                            print(f"    ‚úÖ Found {comment_count} comment divs so far...")
                            
                except StaleElementReferenceException:
                    print(f"    ‚ö†Ô∏è Div became stale during check, skipping...")
                    continue
                except Exception as e:
                    continue
            
            print(f"‚úÖ Extracted {len(fresh_elements)} fresh comment elements")
            return fresh_elements
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting fresh elements: {e}")
            return []

    def extract_all_fresh_comments(self):
        """
        Extract all fresh comments from current page state
        Returns:
            list: Fresh comment elements
        """
        print("üîÑ Extracting ALL fresh comments from current page...")
        fresh_elements = []
        
        try:
            # Strategy 1: Find comments using multiple selectors
            comment_selectors = []
            
            if self.current_layout == "www":
                comment_selectors = [
                    "//div[@role='article']",
                    "//div[contains(@aria-label, 'Comment by')]",
                    "//div[contains(@aria-label, 'B√¨nh lu·∫≠n c·ªßa')]",
                    "//div[.//a[contains(@href, 'facebook.com/') and not(contains(@href, 'groups/') or contains(@href, 'pages/') or contains(@href, 'events/'))]]"
                ]
            elif self.current_layout == "mobile":
                comment_selectors = [
                    "//div[@data-sigil='comment']",
                    "//div[contains(@data-ft, 'comment')]",
                    "//div[contains(@id, 'comment_')]",
                    "//div[.//a[contains(@href, 'profile.php') or contains(@href, 'user.php')]]"
                ]
            else:  # mbasic
                comment_selectors = [
                    "//div[@data-ft and contains(@data-ft, 'comment')]",
                    "//div[contains(@id, 'comment_')]",
                    "//div[.//a[contains(@href, 'profile.php?id=')]]"
                ]
            
            # Extract using each selector
            for i, selector in enumerate(comment_selectors):
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    print(f"  Selector {i+1}: Found {len(elements)} elements")
                    
                    for elem in elements:
                        try:
                            # Quick validation
                            text = safe_get_element_text(elem)
                            if len(text) > 10 and elem not in fresh_elements:
                                fresh_elements.append(elem)
                        except:
                            continue
                            
                except Exception as e:
                    print(f"  Selector {i+1} failed: {e}")
                    continue
            
            # Remove duplicates and sort by position
            unique_elements = []
            seen_locations = set()
            
            for elem in fresh_elements:
                try:
                    location = elem.location
                    location_key = f"{location['x']}_{location['y']}"
                    
                    if location_key not in seen_locations:
                        unique_elements.append(elem)
                        seen_locations.add(location_key)
                        
                except:
                    continue
            
            # Sort by position (top to bottom)
            try:
                unique_elements.sort(key=lambda x: (x.location['y'], x.location['x']))
            except:
                pass
            
            print(f"‚úÖ Final fresh elements: {len(unique_elements)} unique comments")
            return unique_elements
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error in extract_all_fresh_comments: {e}")
            return []

    def is_comment_div(self, div_element):
        """Check if a div element contains comment-like content with stale element protection"""
        try:
            # Get text content safely
            text = safe_get_element_text(div_element)
            if len(text) < 10:  # Too short to be a meaningful comment
                return False
            
            # Check for profile links (common in comments) safely
            profile_links = safe_find_elements(div_element, By.XPATH, ".//a[contains(@href, 'profile') or contains(@href, 'user') or contains(@href, 'facebook.com/')]")
            if profile_links:
                return True
            
            # Check for comment-specific attributes safely
            aria_label = safe_get_element_attribute(div_element, 'aria-label')
            if 'comment' in aria_label.lower() or 'b√¨nh lu·∫≠n' in aria_label.lower():
                return True
            
            # Check for comment-specific classes safely
            class_attr = safe_get_element_attribute(div_element, 'class')
            if any(keyword in class_attr.lower() for keyword in ['comment', 'reply', 'response']):
                return True
            
            # Check for comment-specific data attributes safely
            data_ft = safe_get_element_attribute(div_element, 'data-ft')
            if 'comment' in data_ft.lower():
                return True
            
            # Check for role attribute safely
            role = safe_get_element_attribute(div_element, 'role')
            if role == 'article':
                return True
            
            # Check for comment ID patterns safely
            element_id = safe_get_element_attribute(div_element, 'id')
            if 'comment' in element_id.lower():
                return True
            
            # Check for time elements (comments often have timestamps) safely
            time_elements = safe_find_elements(div_element, By.XPATH, ".//time | .//span[contains(@class, 'time')] | .//a[contains(@class, 'time')]")
            if time_elements:
                return True
            
            # Check for like/reply buttons (common in comments) safely
            action_buttons = safe_find_elements(div_element, By.XPATH, ".//*[contains(text(), 'Like') or contains(text(), 'Reply') or contains(text(), 'Th√≠ch') or contains(text(), 'Tr·∫£ l·ªùi')]")
            if action_buttons:
                return True
            
            # Check for reasonable text length and structure
            if len(text) > 20 and ('@' in text or '.' in text or '!' in text or '?' in text):
                return True
            
            return False
            
        except Exception as e:
            print(f"Error checking if div is comment: {e}")
            return False

    def extract_groups_comments(self):
        """FOCUSED comment extraction v·ªõi immediate processing ƒë·ªÉ tr√°nh stale elements"""
        print(f"=== EXTRACTING GROUPS COMMENTS (FOCUSED + IMMEDIATE) ===")
        
        # Initialize results
        all_comments_data = []
        seen_content = set()

        # Find "All comments" button's parent with class html-div
        try:
            # Use the all_comments_button from _switch_to_all_comments if available
            all_comments_button = getattr(self, 'all_comments_button', None)
            
            if not all_comments_button:
                print("‚ö†Ô∏è No 'All comments' button found from previous method, searching again...")
                # Look for "All comments" button with various possible text variations
                all_comments_selectors = [
                    "//button[contains(text(), 'All comments')]",
                    "//a[contains(text(), 'All comments')]",
                    "//span[contains(text(), 'All comments')]",
                    "//div[contains(text(), 'All comments')]",
                    "//*[contains(text(), 'View all comments')]",
                    "//*[contains(text(), 'See all comments')]",
                    "//*[contains(text(), 'Show all comments')]"
                ]
                
                for selector in all_comments_selectors:
                    try:
                        elements = self.driver.find_elements(By.XPATH, selector)
                        if elements:
                            all_comments_button = elements[0]
                            print(f"‚úÖ Found 'All comments' button using selector: {selector}")
                            break
                    except Exception as e:
                        continue
            else:
                print("‚úÖ Using 'All comments' button from _switch_to_all_comments method")
            
            if all_comments_button:
                # Find the parent with class html-div
                parent_with_html_div = None
                
                # Method 1: Get the closest parent with class containing 'html-div'
                try:
                    closest_parent = all_comments_button.find_element(By.XPATH, "ancestor::*[contains(@class, 'html-div')][1]")
                    if closest_parent:
                        parent_with_html_div = closest_parent
                        print("‚úÖ Found closest parent with html-div class")
                except:
                    pass
                
                # Method 2: Look for immediate parent with class containing 'html-div' (fallback)
                if not parent_with_html_div:
                    try:
                        parent = all_comments_button.find_element(By.XPATH, "./..")
                        if 'html-div' in parent.get_attribute('class') or 'html-div' in parent.get_attribute('className'):
                            parent_with_html_div = parent
                            print("‚úÖ Found parent with html-div class (immediate parent)")
                    except:
                        pass
                
                # Method 3: Look for any ancestor with class containing 'html-div' (fallback)
                if not parent_with_html_div:
                    try:
                        ancestors = all_comments_button.find_elements(By.XPATH, "ancestor::*[contains(@class, 'html-div')]")
                        if ancestors:
                            parent_with_html_div = ancestors[0]
                            print("‚úÖ Found parent with html-div class (ancestor)")
                    except:
                        pass
                
                # Method 4: Look for any div with class containing 'html-div' that contains the button (fallback)
                if not parent_with_html_div:
                    try:
                        # More efficient: search only in the document body
                        html_div_containers = self.driver.find_elements(By.XPATH, "//*[contains(@class, 'html-div')]")
                        for container in html_div_containers:
                            try:
                                # Check if the button is a descendant of this container
                                if container.find_element(By.XPATH, f".//*[contains(@text, '{all_comments_button.text}') or contains(@aria-label, '{all_comments_button.text}')]"):
                                    parent_with_html_div = container
                                    print("‚úÖ Found parent with html-div class (container fallback)")
                                    break
                            except:
                                continue
                    except:
                        pass
                
                if parent_with_html_div:
                    print(f"‚úÖ Successfully found 'All comments' button's parent with html-div class")
                    print(f"   Parent tag: {parent_with_html_div.tag_name}")
                    print(f"   Parent class: {parent_with_html_div.get_attribute('class')}")
                    
                    # Get comment parent divs that come after the "All comments" parent_with_html_div
                    print("üîç Searching for comment parent divs after 'All comments' parent...")
                    
                    comment_parent_divs = []
                    
                    # Method 1: Find the next div that comes immediately after the parent_with_html_div
                    try:
                        # Get only the next div that is a sibling of the parent_with_html_div
                        next_div = parent_with_html_div.find_element(By.XPATH, "./following-sibling::div[1]")
                        print(f"Found next div after parent_with_html_div")
                        print(f"Next div class: {next_div.get_attribute('class')}")
                        
                        # OPTIMIZED click loop v·ªõi performance improvements
                        print("üöÄ Starting optimized 'View more comments' click loop...")
                        previous_comment_count = 0
                        no_new_comments_count = 0
                        max_no_new_comments = 2  # Gi·∫£m t·ª´ 3 xu·ªëng 2
                        max_click_rounds = 10  # Gi·ªõi h·∫°n t·ªëi ƒëa 10 rounds
                        click_round = 0
                        
                        while no_new_comments_count < max_no_new_comments and click_round < max_click_rounds:
                            click_round += 1
                            print(f"\n--- Click Round {click_round}/{max_click_rounds} ---")
                            
                            # Look for "View more comments" button
                            view_more_selectors = [
                                "//button[contains(text(), 'View more comments')]",
                                "//a[contains(text(), 'View more comments')]",
                                "//span[contains(text(), 'View more comments')]",
                                "//div[contains(text(), 'View more comments')]",
                                "//*[contains(text(), 'View more')]",
                                "//*[contains(text(), 'Show more comments')]",
                                "//*[contains(text(), 'Load more comments')]",
                                "//*[contains(text(), 'See more comments')]"
                            ]
                            
                            view_more_button = None
                            for selector in view_more_selectors:
                                try:
                                    elements = self.driver.find_elements(By.XPATH, selector)
                                    if elements:
                                        view_more_button = elements[0]
                                        print(f"‚úÖ Found 'View more comments' button using selector: {selector}")
                                        break
                                except Exception as e:
                                    continue
                            
                            if view_more_button:
                                try:
                                    self.driver.execute_script("arguments[0].click();", view_more_button)
                                    print("üñ±Ô∏è Clicked 'View more comments' button")
                                except Exception as e:
                                    print(f"‚ö†Ô∏è Error clicking 'View more comments' button: {e}")
                                    break
                            else:
                                print("‚ö†Ô∏è No 'View more comments' button found")
                                no_new_comments_count += 1
                                print(f"‚ö†Ô∏è No new comments button detected ({no_new_comments_count}/{max_no_new_comments})")
                                break
                            
                            # Wait for new comments to load (optimized)
                            print("‚è≥ Waiting 3 seconds for new comments to load...")
                            time.sleep(3)  # Gi·∫£m t·ª´ 5s xu·ªëng 3s
                            
                            # RE-FIND fresh container v√† extract immediately
                            processed_in_this_round = 0
                            current_comment_divs = []
                            
                            try:
                                # Re-find parent v√† next_div ƒë·ªÉ tr√°nh stale
                                fresh_parent = self.driver.find_element(By.XPATH, "//*[contains(@class, 'html-div')]")
                                fresh_next_div = fresh_parent.find_element(By.XPATH, "./following-sibling::div[1]")
                                fresh_children = fresh_next_div.find_elements(By.XPATH, "./div")
                                
                                print(f"üîÑ Re-found fresh container with {len(fresh_children)} children")
                                
                                for child_index, child in enumerate(fresh_children):
                                    if self.is_comment_div(child):
                                        try:
                                            # FAST extraction (skip UID resolution trong immediate processing)
                                            comment_data = self.extract_comment_data_fast(child, len(all_comments_data))
                                            
                                            if comment_data:
                                                # Check anonymous v√† duplicates ngay
                                                if comment_data['Name'] != "Unknown" and not is_anonymous_user(comment_data['Name']):
                                                    content_signature = f"{comment_data['Name']}_{comment_data['ProfileLink']}"
                                                    if content_signature not in seen_content:
                                                        seen_content.add(content_signature)
                                                        comment_data['Type'] = 'Comment'
                                                        comment_data['Layout'] = self.current_layout
                                                        comment_data['Source'] = f'Immediate Round {click_round}'
                                                        all_comments_data.append(comment_data)
                                                        processed_in_this_round += 1
                                                        print(f"‚úÖ IMMEDIATE: Added {comment_data['Name']}")
                                                    else:
                                                        print(f"‚úó IMMEDIATE: Duplicate {comment_data['Name']}")
                                                else:
                                                    if comment_data['Name'] != "Unknown" and is_anonymous_user(comment_data['Name']):
                                                        print(f"üö´ IMMEDIATE: Filtered anonymous {comment_data['Name']}")
                                                        self._anonymous_filtered_count += 1
                                            
                                            current_comment_divs.append(child)
                                            
                                        except Exception as extract_error:
                                            print(f"‚ö†Ô∏è IMMEDIATE extraction error: {extract_error}")
                                            current_comment_divs.append(child)
                                            continue
                                
                            except Exception as container_error:
                                print(f"‚ö†Ô∏è Error re-finding fresh container: {container_error}")
                                # Fallback: try global extraction for this round
                                try:
                                    global_elements = self.extract_all_fresh_comments()
                                    print(f"üîÑ Fallback: Found {len(global_elements)} global elements")
                                    
                                    for elem in global_elements[-10:]:  # Process last 10 (likely new ones)
                                        try:
                                            comment_data = self.extract_comment_data_fast(elem, len(all_comments_data))
                                            if comment_data and comment_data['Name'] != "Unknown":
                                                if not is_anonymous_user(comment_data['Name']):
                                                    content_signature = f"{comment_data['Name']}_{comment_data['ProfileLink']}"
                                                    if content_signature not in seen_content:
                                                        seen_content.add(content_signature)
                                                        comment_data['Source'] = f'Global Fallback Round {click_round}'
                                                        all_comments_data.append(comment_data)
                                                        processed_in_this_round += 1
                                                        print(f"‚úÖ FALLBACK: Added {comment_data['Name']}")
                                                else:
                                                    self._anonymous_filtered_count += 1
                                        except:
                                            continue
                                    
                                except Exception as global_error:
                                    print(f"‚ö†Ô∏è Global fallback also failed: {global_error}")
                                    break
                            
                            current_comment_count = len(current_comment_divs)
                            print(f"üìä Round {click_round}: {current_comment_count} divs found")
                            print(f"‚úÖ Processed {processed_in_this_round} new comments in this round")
                            print(f"üìä Total processed so far: {len(all_comments_data)} comments")
                            
                            # Check progress
                            if processed_in_this_round > 0:
                                print(f"‚úÖ Progress made in round {click_round}!")
                                no_new_comments_count = 0  # Reset counter
                            else:
                                no_new_comments_count += 1
                                print(f"‚ö†Ô∏è No progress in round {click_round} ({no_new_comments_count}/{max_no_new_comments})")
                            
                            # EARLY EXIT: Dynamic based on performance
                            early_exit_threshold = 30 if click_round > 5 else 50  # Gi·∫£m threshold sau 5 rounds
                            if len(all_comments_data) >= early_exit_threshold:
                                print(f"üéØ Early exit: ƒê√£ c√≥ {len(all_comments_data)} comments (threshold: {early_exit_threshold})")
                                break
                            
                            # Check for stop flag
                            if self._stop_flag:
                                print("‚èπÔ∏è Stop flag detected, breaking click loop")
                                break
                        
                        print(f"üèÅ Click loop completed. Final comment count: {current_comment_count}")
                        print(f"üìä IMMEDIATE processing results: {len(all_comments_data)} comments extracted during clicks")
                        
                        # Return immediate results (ƒë√£ ƒë∆∞·ª£c process trong loop)
                        if len(all_comments_data) > 0:
                            print(f"\n=== IMMEDIATE EXTRACTION COMPLETE: {len(all_comments_data)} comments ===")
                            return all_comments_data
                        
                        # FALLBACK: N·∫øu immediate processing kh√¥ng c√≥ k·∫øt qu·∫£, th·ª≠ fresh extraction
                        print("‚ö†Ô∏è No results from immediate processing, trying fresh extraction...")
                        fresh_comment_elements = self.extract_all_fresh_comments()

                        if len(fresh_comment_elements) > 0:
                            print(f"üéØ Processing {len(fresh_comment_elements)} fresh comment elements")
                            # Extract comment data from the fresh elements
                            comments_data = []
                            fresh_seen_content = set()
                            
                            for i, element in enumerate(fresh_comment_elements):
                                if self._stop_flag:
                                    break
                                    
                                try:
                                    print(f"\n--- Processing fresh comment element {i+1}/{len(fresh_comment_elements)} ---")
                                    
                                    comment_data = self.extract_comment_data_focused(element, i)
                                    
                                    if not comment_data:
                                        continue
                                    
                                    # Deduplication + Anonymous filter
                                    if comment_data['Name'] == "Unknown":
                                        print("  ‚úó Skipped: no username found")
                                        continue
                                    
                                    # üö´ B·ªé QUA NG∆Ø·ªúI D√ôNG ·∫®N DANH
                                    if is_anonymous_user(comment_data['Name']):
                                        print(f"  üö´ Skipped anonymous user: {comment_data['Name']}")
                                        self._anonymous_filtered_count += 1
                                        continue
                                        
                                    # Check for duplicates
                                    content_signature = f"{comment_data['Name']}_{comment_data['ProfileLink']}"
                                    if content_signature in fresh_seen_content:
                                        print("  ‚úó Skipped: duplicate user")
                                        continue
                                    fresh_seen_content.add(content_signature)
                                    
                                    comment_data['Type'] = 'Comment'
                                    comment_data['Layout'] = self.current_layout
                                    comment_data['Source'] = 'Fresh Extraction Fallback'
                                    
                                    comments_data.append(comment_data)
                                    print(f"  ‚úÖ Added: {comment_data['Name']} - Profile: {comment_data['ProfileLink'][:50]}...")
                                    
                                except Exception as e:
                                    print(f"  Error processing fresh element {i}: {e}")
                                    continue
                            
                            print(f"\n=== FRESH FALLBACK EXTRACTION COMPLETE: {len(comments_data)} comments ===")
                            return comments_data
                            
                    except Exception as e:
                        print(f"Error finding next div: {e}")
                    
                    print(f"üéØ Total comment parent divs found after 'All comments': {len(comment_parent_divs)}")
                    # Extract comment data from the found divs
                    comments_data = []
                    seen_content = set()
                    
                    for i, element in enumerate(comment_parent_divs):
                        if self._stop_flag:
                            break
                            
                        try:
                            print(f"\n--- Processing comment div {i+1}/{len(comment_parent_divs)} ---")
                            
                            comment_data = self.extract_with_retry(element, i)
                            
                            if not comment_data:
                                continue
                            
                            # Deduplication + Anonymous filter
                            if comment_data['Name'] == "Unknown":
                                print("  ‚úó Skipped: no username found")
                                continue
                            
                            # üö´ B·ªé QUA NG∆Ø·ªúI D√ôNG ·∫®N DANH
                            if is_anonymous_user(comment_data['Name']):
                                print(f"  üö´ Skipped anonymous user: {comment_data['Name']}")
                                self._anonymous_filtered_count += 1
                                continue
                                
                            # Check for duplicates
                            content_signature = f"{comment_data['Name']}_{comment_data['ProfileLink']}"
                            if content_signature in seen_content:
                                print("  ‚úó Skipped: duplicate user")
                                continue
                            seen_content.add(content_signature)
                            
                            comment_data['Type'] = 'Comment'
                            comment_data['Layout'] = self.current_layout
                            comment_data['Source'] = 'All Comments Container'
                            
                            comments_data.append(comment_data)
                            print(f"  ‚úÖ Added: {comment_data['Name']} - Profile: {comment_data['ProfileLink'][:50]}...")
                            
                        except Exception as e:
                            print(f"  Error processing comment div {i}: {e}")
                            continue
                    
                    print(f"\n=== EXTRACTION COMPLETE: {len(comments_data)} comments ===")
                    return comments_data
                else:
                    print("‚ùå Could not find parent with html-div class for 'All comments' button")
                    
            else:
                print("‚ùå Could not find 'All comments' button")
                
        except Exception as e:
            print(f"‚ùå Error while searching for 'All comments' button's parent: {e}")
        
        # Save page for debugging
        try:
            with open(f"debug_focused_{self.current_layout}.html", "w", encoding="utf-8") as f:
                f.write(self.driver.page_source)
            print(f"Saved page to debug_focused_{self.current_layout}.html")
        except:
            pass
        
        # FALLBACK: Use fresh extraction if no comments found above
        print("üîÑ Using fresh extraction strategy to avoid stale elements...")
        
        # Extract all fresh comments from current page
        all_comment_elements = self.extract_all_fresh_comments()
        
        if len(all_comment_elements) == 0:
            print("‚ö†Ô∏è No comments found with fresh extraction, trying fallback selectors...")
            
            fallback_selectors = [
                "//div[.//a[contains(@href, 'facebook.com/')] and string-length(normalize-space(text())) > 20]",
                "//div[string-length(normalize-space(text())) > 30]",
                "//*[.//a[contains(@href, 'profile')] and string-length(normalize-space(text())) > 15]"
            ]
            
            for selector in fallback_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    print(f"Fallback selector: Found {len(elements)} elements")
                    for elem in elements:
                        try:
                            text = safe_get_element_text(elem)
                            if len(text) > 10 and elem not in all_comment_elements:
                                all_comment_elements.append(elem)
                        except:
                            continue
                    
                    if len(all_comment_elements) > 20:
                        break
                except:
                    continue
        
        comments = []
        seen_content = set()
        
        print(f"Processing {len(all_comment_elements)} fresh comment elements...")
        
        # Process each fresh element
        for i, element in enumerate(all_comment_elements):
            if self._stop_flag:
                break
                
            try:
                print(f"\n--- Fresh Element {i+1}/{len(all_comment_elements)} ---")
                
                comment_data = self.extract_comment_data_focused(element, i)
                
                if not comment_data:
                    continue
                
                # Deduplication + Anonymous filter
                if comment_data['Name'] == "Unknown":
                    print("  ‚úó Skipped: no username found")
                    continue
                
                # üö´ B·ªé QUA NG∆Ø·ªúI D√ôNG ·∫®N DANH
                if is_anonymous_user(comment_data['Name']):
                    print(f"  üö´ Skipped anonymous user: {comment_data['Name']}")
                    self._anonymous_filtered_count += 1
                    continue
                    
                # Check for duplicates
                content_signature = f"{comment_data['Name']}_{comment_data['ProfileLink']}"
                if content_signature in seen_content:
                    print("  ‚úó Skipped: duplicate user")
                    continue
                seen_content.add(content_signature)
                
                comment_data['Type'] = 'Comment'
                comment_data['Layout'] = self.current_layout
                comment_data['Source'] = 'Fresh Extraction (STALE-FREE)'
                
                comments.append(comment_data)
                print(f"  ‚úÖ Added: {comment_data['Name']} - Profile: {comment_data['ProfileLink'][:50]}...")
                
            except StaleElementReferenceException:
                print(f"  ‚ö†Ô∏è Element {i+1} became stale during processing, skipping...")
                continue
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error processing fresh element {i+1}: {e}")
                continue
        
        print(f"\n=== FRESH EXTRACTION COMPLETE: {len(comments)} comments ===")
        return comments

    def extract_comment_data_focused(self, element, index):
        """FOCUSED comment data extraction v·ªõi enhanced UID resolution v√† stale element handling"""
        try:
            # Safe text extraction with stale element handling
            full_text = safe_get_element_text(element)
            if len(full_text) < 5:
                print(f"  ‚ùå Text too short: '{full_text}'")
                return None
            
            print(f"  Processing: '{full_text[:60]}...'")
            
            username = "Unknown"
            profile_href = ""
            uid = "Unknown"
            
            # FOCUSED: Enhanced username extraction with stale element protection
            print(f"    üéØ FOCUSED analysis of element structure...")
            
            # Method 1: Get ALL links and analyze each one with safe methods
            try:
                all_links = safe_find_elements(element, By.XPATH, ".//a")
                print(f"    Found {len(all_links)} total links in element")
                
                for link_index, link in enumerate(all_links):
                    try:
                        link_text = safe_get_element_text(link)
                        link_href = safe_get_element_attribute(link, "href")
                        
                        print(f"      Link {link_index+1}: Text='{link_text}' | Href={link_href[:60]}...")
                        
                        # Check if this is a Facebook profile link
                        if ('facebook.com' in link_href and 
                            ('profile.php' in link_href or '/user/' in link_href or 'user.php' in link_href or 
                             (not any(x in link_href for x in ['groups', 'pages', 'events', 'photo', 'video'])))):
                            
                            # Enhanced name validation v·ªõi anonymous filter
                            if (link_text and 
                                len(link_text) >= 2 and 
                                len(link_text) <= 100 and
                                not link_text.isdigit() and
                                not link_text.startswith('http') and
                                not is_anonymous_user(link_text) and  # üö´ B·ªé QUA NG∆Ø·ªúI D√ôNG ·∫®N DANH
                                not any(ui in link_text.lower() for ui in [
                                    'like', 'reply', 'share', 'comment', 'th√≠ch', 'tr·∫£ l·ªùi', 
                                    'chia s·∫ª', 'b√¨nh lu·∫≠n', 'ago', 'tr∆∞·ªõc', 'min', 'hour', 
                                    'day', 'ph√∫t', 'gi·ªù', 'ng√†y', 
                                    'view', 'xem', 'show', 'hi·ªÉn th·ªã', 'see more', 'view more'
                                ])):
                                
                                username = link_text
                                profile_href = link_href
                                
                                # Extract UID from URL tr∆∞·ªõc
                                uid = extract_uid_from_profile_url(link_href)
                                
                                # N·∫øu UID v·∫´n ch∆∞a c√≥ ho·∫∑c l√† username, th·ª≠ resolve
                                if uid == "Unknown" or uid.startswith("username:"):
                                    if uid.startswith("username:"):
                                        username_to_resolve = uid.split(":", 1)[1]
                                    else:
                                        username_to_resolve = username
                                    
                                    print(f"      üîÑ Attempting to resolve UID for: {username_to_resolve}")
                                    resolved_uid = get_uid_from_username(username_to_resolve, self.cookies_dict, self.driver)
                                    if resolved_uid != "Unknown":
                                        uid = resolved_uid
                                        print(f"      ‚úÖ Successfully resolved UID: {uid}")
                                    else:
                                        print(f"      ‚ö†Ô∏è Could not resolve UID for: {username_to_resolve}")
                                
                                print(f"      ‚úÖ FOCUSED: Found valid profile: {username} -> UID: {uid}")
                                break
                                
                    except Exception as e:
                        print(f"      Error processing link {link_index+1}: {e}")
                        continue
                
            except Exception as e:
                print(f"    Error in focused method: {e}")
            
            # Fallback: If no username from links, try using the first line of the first child element's text
            if username == "Unknown":
                try:
                    children = safe_find_elements(element, By.XPATH, "./*")
                    if children:
                        first_child_text = safe_get_element_text(children[0])
                        if first_child_text:
                            first_line = first_child_text.splitlines()[0].strip()
                            # Basic validation for a plausible name line + anonymous check
                            if (first_line and 
                                2 <= len(first_line) <= 120 and 
                                not first_line.startswith("http") and
                                not is_anonymous_user(first_line)):  # üö´ B·ªé QUA NG∆Ø·ªúI D√ôNG ·∫®N DANH
                                
                                username = first_line
                                print(f"      ‚úÖ Fallback name from first child: {username}")
                                
                                # Th·ª≠ resolve UID t·ª´ username fallback
                                print(f"      üîÑ Attempting to resolve UID for fallback username: {username}")
                                resolved_uid = get_uid_from_username(username, self.cookies_dict, self.driver)
                                if resolved_uid != "Unknown":
                                    uid = resolved_uid
                                    print(f"      ‚úÖ Successfully resolved UID from fallback: {uid}")
                            else:
                                if first_line and is_anonymous_user(first_line):
                                    print(f"      üö´ Skipped anonymous fallback username: {first_line}")
                                
                except Exception as e:
                    print(f"      ‚ö†Ô∏è Fallback name extraction error: {e}")

            # Final validation
            if username == "Unknown":
                print("  ‚ùå FOCUSED extraction failed for this element")
                return None
                
            print(f"  ‚úÖ FOCUSED: Successfully extracted username: {username} | UID: {uid}")
            
            return {
                "UID": uid,
                "Name": username,
                "ProfileLink": profile_href,
                "CommentLink": "",
                "ElementIndex": index,
                "TextPreview": full_text[:100] + "..." if len(full_text) > 100 else full_text,
                "ContainerHeight": "Focused on larger height container"
            }
            
        except Exception as e:
            print(f"Error in focused extraction: {e}")
            return None

    def extract_comment_data_fast(self, element, index):
        """FAST comment data extraction WITHOUT UID resolution (for immediate processing)"""
        try:
            # Safe text extraction
            full_text = safe_get_element_text(element)
            if len(full_text) < 5:
                return None
            
            username = "Unknown"
            profile_href = ""
            
            # FAST: Enhanced username extraction WITHOUT UID resolution
            try:
                all_links = safe_find_elements(element, By.XPATH, ".//a")
                
                for link in all_links:
                    try:
                        link_text = safe_get_element_text(link)
                        link_href = safe_get_element_attribute(link, "href")
                        
                        # Check if this is a Facebook profile link
                        if ('facebook.com' in link_href and 
                            ('profile.php' in link_href or '/user/' in link_href or 'user.php' in link_href or 
                             (not any(x in link_href for x in ['groups', 'pages', 'events', 'photo', 'video'])))):
                            
                            # Enhanced name validation v·ªõi anonymous filter
                            if (link_text and 
                                len(link_text) >= 2 and 
                                len(link_text) <= 100 and
                                not link_text.isdigit() and
                                not link_text.startswith('http') and
                                not is_anonymous_user(link_text) and  # üö´ B·ªé QUA NG∆Ø·ªúI D√ôNG ·∫®N DANH
                                not any(ui in link_text.lower() for ui in [
                                    'like', 'reply', 'share', 'comment', 'th√≠ch', 'tr·∫£ l·ªùi', 
                                    'chia s·∫ª', 'b√¨nh lu·∫≠n', 'ago', 'tr∆∞·ªõc', 'min', 'hour', 
                                    'day', 'ph√∫t', 'gi·ªù', 'ng√†y', 
                                    'view', 'xem', 'show', 'hi·ªÉn th·ªã', 'see more', 'view more'
                                ])):
                                
                                username = link_text
                                profile_href = link_href
                                break
                                
                    except Exception as e:
                        continue
                
            except Exception as e:
                pass
            
            # Fallback: First child text (without UID resolution)
            if username == "Unknown":
                try:
                    children = safe_find_elements(element, By.XPATH, "./*")
                    if children:
                        first_child_text = safe_get_element_text(children[0])
                        if first_child_text:
                            first_line = first_child_text.splitlines()[0].strip()
                            if (first_line and 
                                2 <= len(first_line) <= 120 and 
                                not first_line.startswith("http") and
                                not is_anonymous_user(first_line)):
                                
                                username = first_line
                                
                except Exception as e:
                    pass

            # Final validation
            if username == "Unknown":
                return None
                
            return {
                "UID": "Unknown",  # Will be resolved later in batch
                "Name": username,
                "ProfileLink": profile_href,
                "CommentLink": "",
                "ElementIndex": index,
                "TextPreview": full_text[:100] + "..." if len(full_text) > 100 else full_text,
                "ContainerHeight": "Fast extraction"
            }
            
        except Exception as e:
            return None

    def batch_resolve_uids(self, comments, max_network_resolves=10):
        """
        OPTIMIZED: Batch resolve UIDs v·ªõi gi·ªõi h·∫°n network calls
        Args:
            comments (list): List of comments
            max_network_resolves (int): Max s·ªë l∆∞·ª£ng network calls
        Returns:
            int: S·ªë UIDs resolved
        """
        print(f"üöÄ BATCH UID resolution for {len(comments)} comments...")
        uid_resolved_count = 0
        network_resolves_used = 0
        
        # Phase 1: Fast URL-based resolution (no network)
        for comment in comments:
            if comment.get('UID') == "Unknown" and comment.get('ProfileLink'):
                fast_uid = extract_uid_from_profile_url(comment['ProfileLink'])
                if fast_uid != "Unknown" and not fast_uid.startswith("username:"):
                    comment['UID'] = fast_uid
                    uid_resolved_count += 1
        
        print(f"‚ö° Phase 1: {uid_resolved_count} UIDs t·ª´ URLs")
        
        # Phase 2: Limited network resolution cho important cases
        if network_resolves_used < max_network_resolves:
            for comment in comments:
                if network_resolves_used >= max_network_resolves:
                    break
                    
                if comment.get('UID') == "Unknown" and comment.get('Name') != "Unknown":
                    # Ch·ªâ resolve network cho users c√≥ profile link
                    if comment.get('ProfileLink'):
                        resolved_uid = get_uid_from_username(comment['Name'], self.cookies_dict, self.driver)
                        if resolved_uid != "Unknown":
                            comment['UID'] = resolved_uid
                            uid_resolved_count += 1
                            network_resolves_used += 1
                            print(f"  üåê Network UID #{network_resolves_used}: {comment['Name']} -> {resolved_uid}")
        
        print(f"üéØ BATCH completed: {uid_resolved_count} total UIDs | {network_resolves_used} network calls")
        return uid_resolved_count

    def scrape_all_comments(self, limit=0, resolve_uid=True, progress_callback=None):
        """Main scraping orchestrator with FOCUSED approach"""
        print(f"=== STARTING FOCUSED GROUPS SCRAPING ===")
        
        # Reset counters
        self._anonymous_filtered_count = 0
        
        if self._stop_flag:
            return []
        
        # Step 1: Extract comments with focus
        comments = self.extract_groups_comments()
        
        # Step 2: Filter out anonymous users BEFORE UID resolution
        if comments:
            print(f"\nüö´ Filtering anonymous users from {len(comments)} comments...")
            filtered_comments = []
            anonymous_count = 0
            
            for comment in comments:
                if comment.get('Name') == "Unknown":
                    continue
                    
                if is_anonymous_user(comment['Name']):
                    anonymous_count += 1
                    print(f"  üö´ Filtered anonymous: {comment['Name']}")
                    continue
                    
                filtered_comments.append(comment)
            
            # Store count for statistics
            self._anonymous_filtered_count = anonymous_count
            
            print(f"  üìä Filtered out {anonymous_count} anonymous users")
            print(f"  ‚úÖ Remaining: {len(filtered_comments)} real users")
            comments = filtered_comments
        
        # Step 3: OPTIMIZED UID resolution v·ªõi batch processing
        if resolve_uid and comments:
            uid_resolved_count = self.batch_resolve_uids(comments, max_network_resolves=5)  # Gi·ªõi h·∫°n 5 network calls
        
        # Step 4: Apply limit
        if limit > 0 and len(comments) > limit:
            comments = comments[:limit]
            print(f"üìä Limited to {limit} comments")
        
        # Step 5: Progress reporting
        if progress_callback:
            progress_callback(len(comments))
        
        # Statistics
        uid_count = len([c for c in comments if c.get('UID', 'Unknown') != 'Unknown'])
        anonymous_filtered = self._anonymous_filtered_count
        uid_rate = (uid_count / len(comments)) * 100 if comments else 0
        print(f"‚úÖ OPTIMIZED scraping completed: {len(comments)} real users | {uid_count} UIDs ({uid_rate:.1f}%) | {anonymous_filtered} anonymous filtered")
        return comments

    def close(self):
        try: 
            self.driver.quit()
        except: 
            pass

# ----------------------------
# FOCUSED GUI
# ----------------------------

class FBGroupsAppGUI:
    def __init__(self, root):
        self.root = root
        root.title("üéØ FB Groups Comment Scraper - FOCUSED + UID")
        root.geometry("1100x950")
        root.configure(bg="#121212")

        # Main frame
        main_frame = tk.Frame(root, bg="#121212")
        main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=15)

        # Header
        header_frame = tk.Frame(main_frame, bg="#121212")
        header_frame.pack(fill="x", pady=(0,20))
        
        title_label = tk.Label(header_frame, text="üéØ FB Groups Scraper - FOCUSED + UID + Anonymous Filter", 
                              font=("Arial", 20, "bold"), bg="#121212", fg="#a5d6a7")
        title_label.pack()
        
        subtitle_label = tk.Label(header_frame, text="üéØ Enhanced: Extracts usernames + UIDs + Skips anonymous users", 
                                 font=("Arial", 11), bg="#121212", fg="#b0b0b0")
        subtitle_label.pack(pady=(5,0))

        # Input section
        input_frame = tk.LabelFrame(main_frame, text="üìù Th√¥ng tin b√†i vi·∫øt Groups", font=("Arial", 12, "bold"), 
                                   bg="#121212", fg="#a5d6a7", relief="groove", bd=2)
        input_frame.pack(fill="x", pady=(0,15))

        tk.Label(input_frame, text="üîó Link b√†i vi·∫øt trong Groups:", bg="#121212", font=("Arial", 10)).pack(anchor="w", padx=15, pady=(15,5))
        self.entry_url = tk.Entry(input_frame, width=100, font=("Arial", 9))
        self.entry_url.pack(fill="x", padx=15, pady=(0,10))

        tk.Label(input_frame, text="üç™ Cookie Facebook (ƒë·ªÉ truy c·∫≠p Groups):", bg="#121212", font=("Arial", 10)).pack(anchor="w", padx=15, pady=(5,5))
        self.txt_cookie = tk.Text(input_frame, height=4, font=("Arial", 8))
        self.txt_cookie.pack(fill="x", padx=15, pady=(0,15))

        # Options section
        options_frame = tk.LabelFrame(main_frame, text="üéØ C·∫•u h√¨nh FOCUSED + UID + Anonymous Filter", font=("Arial", 12, "bold"), 
                                     bg="#121212", fg="#a5d6a7", relief="groove", bd=2)
        options_frame.pack(fill="x", pady=(0,15))
        
        opt_grid = tk.Frame(options_frame, bg="#121212")
        opt_grid.pack(fill="x", padx=15, pady=15)
        
        # Options grid
        tk.Label(opt_grid, text="üìä S·ªë l∆∞·ª£ng comment:", bg="#121212").grid(row=0, column=0, sticky="w")
        self.entry_limit = tk.Entry(opt_grid, width=10)
        self.entry_limit.insert(0, "0")
        self.entry_limit.grid(row=0, column=1, sticky="w", padx=(10,20))
        tk.Label(opt_grid, text="(0 = t·∫•t c·∫£)", bg="#121212", fg="#6c757d").grid(row=0, column=2, sticky="w")

        self.headless_var = tk.BooleanVar(value=False)  # Default to visible for debugging
        tk.Checkbutton(opt_grid, text="üëª Ch·∫°y ·∫©n", variable=self.headless_var,
                      bg="#121212", font=("Arial", 9)).grid(row=1, column=0, sticky="w", pady=(10,0))

        self.resolve_uid_var = tk.BooleanVar(value=False)  # Default False cho speed
        tk.Checkbutton(opt_grid, text="üÜî L·∫•y UID t·ª´ username (ch·∫≠m h∆°n)", variable=self.resolve_uid_var, 
                      bg="#121212", font=("Arial", 9)).grid(row=1, column=1, sticky="w", pady=(10,0))

        # Speed mode option
        self.speed_mode_var = tk.BooleanVar(value=True)
        tk.Checkbutton(opt_grid, text="‚ö° Speed mode (gi·ªõi h·∫°n 50 comments)", variable=self.speed_mode_var,
                      bg="#121212", font=("Arial", 9)).grid(row=2, column=0, sticky="w", pady=(5,0))

        # Th√™m note v·ªÅ anonymous filter
        tk.Label(opt_grid, text="üö´ T·ª± ƒë·ªông b·ªè qua ng∆∞·ªùi d√πng ·∫©n danh", bg="#121212", fg="#ffc107", 
                font=("Arial", 9, "italic")).grid(row=3, column=0, columnspan=3, sticky="w", pady=(5,0))

        # File section
        file_frame = tk.LabelFrame(main_frame, text="üíæ Xu·∫•t k·∫øt qu·∫£", font=("Arial", 12, "bold"), 
                                  bg="#121212", fg="#a5d6a7", relief="groove", bd=2)
        file_frame.pack(fill="x", pady=(0,15))
        
        file_row = tk.Frame(file_frame, bg="#121212")
        file_row.pack(fill="x", padx=15, pady=15)
        
        self.entry_file = tk.Entry(file_row, width=70, font=("Arial", 9))
        current_date = datetime.now().strftime("%d_%m_%Y")
        self.entry_file.insert(0, f"facebook_groups_comments_UID_NoAnonymous_{current_date}.xlsx")
        self.entry_file.pack(side="left", fill="x", expand=True)
        
        self.btn_choose = tk.Button(file_row, text="üìÅ Ch·ªçn", command=self.choose_file, 
                 bg="#17a2b8", fg="black", font=("Arial", 9))
        self.btn_choose.pack(side="right", padx=(10,0))

        # Status section
        status_frame = tk.LabelFrame(main_frame, text="üìä Tr·∫°ng th√°i th·ª±c thi - ENHANCED UID + ANONYMOUS FILTER", font=("Arial", 12, "bold"), 
                                    bg="#121212", fg="#a5d6a7", relief="groove", bd=2)
        status_frame.pack(fill="x", pady=(0,15))
        
        self.lbl_status = tk.Label(status_frame, text="‚úÖ Enhanced UID + Anonymous Filter scraper s·∫µn s√†ng - T·ª± ƒë·ªông b·ªè qua ng∆∞·ªùi d√πng ·∫©n danh", fg="#28a745", 
                                  wraplength=900, justify="left", font=("Arial", 11), bg="#121212")
        self.lbl_status.pack(anchor="w", padx=15, pady=(15,5))

        self.lbl_progress_detail = tk.Label(status_frame, text="üí° NEW: Username ‚Üí UID conversion | Anonymous filtering | Stale element protection | Enhanced debugging",
                                          fg="#b0b0b0", wraplength=900, justify="left", font=("Arial", 9), bg="#121212")
        self.lbl_progress_detail.pack(anchor="w", padx=15, pady=(0,10))

        # Progress bar
        self.progress_bar = ttk.Progressbar(status_frame, mode='indeterminate')
        self.progress_bar.pack(fill="x", padx=15, pady=(0,15))

        # Control buttons
        button_frame = tk.Frame(main_frame, bg="#121212")
        button_frame.pack(fill="x", pady=20)
        
        self.btn_start = tk.Button(button_frame, text="üöÄ B·∫Øt ƒë·∫ßu UID + Filter Scraping", bg="#28a745", fg="black", 
                                  font=("Arial", 14, "bold"), command=self.start_scrape_thread, 
                                  pady=12, padx=40)
        self.btn_start.pack(side="left")

        self.btn_stop = tk.Button(button_frame, text="‚èπÔ∏è D·ª´ng", bg="#dc3545", fg="black", 
                                 font=("Arial", 14, "bold"), command=self.stop_scrape, 
                                 state=tk.DISABLED, pady=12, padx=40)
        self.btn_stop.pack(side="left", padx=(25,0))

        self.progress_var = tk.IntVar(value=0)
        self.progress_label = tk.Label(button_frame, textvariable=self.progress_var, fg="#28a745", 
                                     font=("Arial", 18, "bold"), bg="#121212")
        self.progress_label.pack(side="right")

        self._scrape_thread = None
        self._stop_flag = False
        self.scraper = None

        # Apply dark theme across widgets
        self._apply_dark_theme()

        # Beautify primary (start) and danger (stop) buttons
        self._beautify_button(self.btn_start, base_bg="#2ecc71", hover_bg="#27ae60", active_bg="#1e874b")
        self._beautify_button(self.btn_stop, base_bg="#e74c3c", hover_bg="#c0392b", active_bg="#992d22")
        # Beautify choose file button with cyan palette
        self._beautify_button(self.btn_choose, base_bg="#17a2b8", hover_bg="#1491a1", active_bg="#0f6f7b")

    def _apply_dark_theme(self):
        """Apply a dark theme recursively to Tk widgets and ttk components."""
        dark_bg = "#121212"
        surface_bg = "#1e1e1e"
        light_fg = "#e0e0e0"
        subtle_fg = "#b0b0b0"

        # Root background
        try:
            self.root.configure(bg=dark_bg)
        except Exception:
            pass

        def apply(widget):
            # Background
            try:
                if 'background' in widget.configure():
                    widget.configure(bg=dark_bg)
            except Exception:
                pass

            # Foreground: only adjust if currently black/dark default
            try:
                # Skip changing foreground for clickable buttons so custom styles persist
                if isinstance(widget, tk.Button):
                    pass
                else:
                    cfg = widget.configure()
                    if 'foreground' in cfg:
                        current_fg = str(widget.cget('fg')).lower()
                        if current_fg in ('black', '#000000'):
                            widget.configure(fg=light_fg)
            except Exception:
                pass

            # Special cases
            try:
                if isinstance(widget, tk.Entry):
                    widget.configure(bg=surface_bg, fg=light_fg, insertbackground=light_fg)
                if isinstance(widget, tk.Text):
                    widget.configure(bg=surface_bg, fg=light_fg, insertbackground=light_fg)
                if isinstance(widget, tk.Listbox):
                    widget.configure(bg=surface_bg, fg=light_fg, selectbackground="#2a2a2a")
            except Exception:
                pass

            for child in widget.winfo_children():
                apply(child)

        apply(self.root)

        # ttk styling (progress bar)
        try:
            style = ttk.Style()
            # Use a theme that allows color customization
            try:
                style.theme_use('clam')
            except Exception:
                pass
            style.configure("TProgressbar",
                            troughcolor=surface_bg,
                            background="#00bcd4",
                            bordercolor=surface_bg,
                            lightcolor=surface_bg,
                            darkcolor=surface_bg)
        except Exception:
            pass

    def _beautify_button(self, button, base_bg="#2ecc71", hover_bg="#27ae60", active_bg="#1e874b"):
        """Apply modern flat styling and hover/active effects to tk.Button."""
        try:
            button.configure(
                bg=base_bg,
                fg="#000000",
                activebackground=active_bg,
                activeforeground="#000000",
                bd=0,
                highlightthickness=0,
                relief="flat",
                cursor="hand2",
                disabledforeground="#777777"
            )

            def on_enter(_):
                try:
                    button.configure(bg=hover_bg)
                except Exception:
                    pass

            def on_leave(_):
                try:
                    button.configure(bg=base_bg)
                except Exception:
                    pass

            def on_press(_):
                try:
                    button.configure(bg=active_bg)
                except Exception:
                    pass

            def on_release(_):
                try:
                    button.configure(bg=hover_bg)
                except Exception:
                    pass

            button.bind("<Enter>", on_enter)
            button.bind("<Leave>", on_leave)
            button.bind("<ButtonPress-1>", on_press)
            button.bind("<ButtonRelease-1>", on_release)
        except Exception:
            pass

    def choose_file(self):
        f = filedialog.asksaveasfilename(
            defaultextension=".xlsx", 
            filetypes=[("Excel files", "*.xlsx"), ("CSV files", "*.csv")],
            title="Ch·ªçn file ƒë·ªÉ l∆∞u Groups comments v·ªõi UID"
        )
        if f:
            self.entry_file.delete(0, tk.END)
            self.entry_file.insert(0, f)

    def start_scrape_thread(self):
        url = self.entry_url.get().strip()
        cookie_str = self.txt_cookie.get("1.0", tk.END).strip()
        file_out = self.entry_file.get().strip() or "facebook_groups_comments_UID.xlsx"
        
        if not url:
            messagebox.showerror("‚ùå L·ªói", "Vui l√≤ng nh·∫≠p link b√†i vi·∫øt Groups.")
            return
        
        if "groups/" not in url:
            result = messagebox.askyesno("‚ö†Ô∏è X√°c nh·∫≠n", 
                                       "Link n√†y c√≥ v·∫ª kh√¥ng ph·∫£i Groups. B·∫°n c√≥ mu·ªën ti·∫øp t·ª•c kh√¥ng?")
            if not result:
                return
        
        try: 
            limit = int(self.entry_limit.get().strip())
        except: 
            limit = 0

        self._stop_flag = False
        self.progress_var.set(0)
        self.progress_bar.start()
        self.lbl_status.config(text="üîÑ ƒêang kh·ªüi ƒë·ªông Enhanced UID Groups scraper...", fg="#fd7e14")
        self.lbl_progress_detail.config(text="‚è≥ Initializing UID extraction with Selenium + Requests methods...")
        self.btn_start.config(state=tk.DISABLED)
        self.btn_stop.config(state=tk.NORMAL)

        self._scrape_thread = threading.Thread(target=self._scrape_worker, 
                                             args=(url, cookie_str, file_out, limit, 
                                                   self.headless_var.get(), self.resolve_uid_var.get(),
                                                   self.speed_mode_var.get()))
        self._scrape_thread.daemon = True
        self._scrape_thread.start()

    def stop_scrape(self):
        self._stop_flag = True
        if self.scraper:
            self.scraper._stop_flag = True
        self.lbl_status.config(text="‚èπÔ∏è ƒêang d·ª´ng UID scraper...", fg="#dc3545")
        self.btn_stop.config(state=tk.DISABLED)

    def _progress_cb(self, count):
        self.progress_var.set(count)
        self.lbl_status.config(text=f"üìà UID processing... ƒê√£ l·∫•y {count} comments", fg="#28a745")
        self.root.update_idletasks()

    def _scrape_worker(self, url, cookie_str, file_out, limit, headless, resolve_uid, speed_mode=True):
        try:
            # Initialize
            self.lbl_status.config(text="üåê Kh·ªüi t·∫°o Enhanced UID Groups scraper...", fg="#fd7e14")
            self.scraper = FacebookGroupsScraper(cookie_str, headless=headless)
            
            if self._stop_flag: return
            
            # Load post
            self.lbl_status.config(text="üìÑ ƒêang t·∫£i b√†i vi·∫øt Groups v·ªõi UID logic...", fg="#fd7e14")
            self.lbl_progress_detail.config(text="‚è≥ Loading post with enhanced UID resolution...")
            success = self.scraper.load_post(url)
            
            if not success:
                self.lbl_status.config(text="‚ùå Kh√¥ng th·ªÉ t·∫£i b√†i vi·∫øt Groups", fg="#dc3545")
                self.lbl_progress_detail.config(text="üí° Ki·ªÉm tra: 1) Cookie valid, 2) Quy·ªÅn truy c·∫≠p Groups, 3) Link ch√≠nh x√°c")
                return
            
            # Show detected layout
            layout = getattr(self.scraper, 'current_layout', 'unknown')
            self.lbl_progress_detail.config(text=f"üéØ Layout detected: {layout} - Using Enhanced UID extraction...")
                
            if self._stop_flag: return
            
            # Scrape with Enhanced UID logic
            mode_text = "‚ö° SPEED MODE" if speed_mode else "üîç FULL MODE"
            self.lbl_status.config(text=f"{mode_text} Groups extraction ({layout})...", fg="#fd7e14")
            
            if speed_mode:
                self.lbl_progress_detail.config(text="‚ö° Speed mode: Fast extraction, limited UID resolution...")
                # Override settings cho speed mode
                actual_limit = min(limit, 50) if limit > 0 else 50
                actual_resolve_uid = False  # Disable UID resolution trong speed mode
            else:
                self.lbl_progress_detail.config(text="‚è≥ Full mode: Complete extraction v·ªõi UID resolution...")
                actual_limit = limit
                actual_resolve_uid = resolve_uid
            
            comments = self.scraper.scrape_all_comments(limit=actual_limit, resolve_uid=actual_resolve_uid, 
                                                       progress_callback=self._progress_cb)
            
            # SPEED MODE: Extract UIDs t·ª´ URLs c√≥ s·∫µn (kh√¥ng c·∫ßn network)
            if speed_mode and comments:
                print(f"\n‚ö° SPEED MODE: Fast UID extraction t·ª´ URLs...")
                url_uid_count = 0
                for comment in comments:
                    if comment.get('UID') == "Unknown" and comment.get('ProfileLink'):
                        fast_uid = extract_uid_from_profile_url(comment['ProfileLink'])
                        if fast_uid != "Unknown" and not fast_uid.startswith("username:"):
                            comment['UID'] = fast_uid
                            url_uid_count += 1
                
                print(f"‚ö° Speed UID extraction: {url_uid_count} UIDs t·ª´ URLs")
            
            print(f"‚úÖ Comments: {len(comments)} | Speed mode: {speed_mode}")

            if self._stop_flag: return
            
            # Save
            self.lbl_status.config(text="üíæ ƒêang l∆∞u Enhanced UID Groups data...", fg="#fd7e14")
            
            if comments:
                df = pd.DataFrame(comments)
                
                # Add metadata
                df.insert(0, 'STT', range(1, len(df) + 1))
                df['Source'] = 'Facebook Groups - Enhanced UID'
                df['ScrapedAt'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # File handling
                if not file_out.lower().endswith((".xlsx", ".csv")):
                    file_out += ".xlsx"
                
                if file_out.lower().endswith(".csv"):
                    df.to_csv(file_out, index=False, encoding="utf-8-sig")
                else:
                    df.to_excel(file_out, index=False, engine="openpyxl")
                
                # Statistics
                unique_users = len(set(c['Name'] for c in comments if c['Name'] != 'Unknown'))
                profile_links = len([c for c in comments if c['ProfileLink']])
                uid_count = len([c for c in comments if c.get('UID', 'Unknown') != 'Unknown'])
                uid_success_rate = (uid_count / len(comments)) * 100 if comments else 0
                anonymous_filtered = getattr(self.scraper, '_anonymous_filtered_count', 0)
                
                mode_emoji = "‚ö°" if speed_mode else "üîç"
                mode_text = "SPEED MODE" if speed_mode else "FULL MODE"
                
                self.lbl_status.config(text=f"üéâ {mode_emoji} {mode_text} SCRAPING HO√ÄN TH√ÄNH!", fg="#28a745")
                self.lbl_progress_detail.config(text=f"üìä {mode_text}: {len(comments)} users | {uid_count} UIDs ({uid_success_rate:.1f}%) | üö´ {anonymous_filtered} anonymous | {layout}")
                
                print(f"üéØ {mode_emoji} {mode_text} SCRAPING COMPLETE!")
                print(f"   üìä Results: {len(comments)} real user comments")
                print(f"   üë• Unique users: {unique_users}")
                print(f"   üîó Profile links: {profile_links}")
                print(f"   üÜî UIDs extracted: {uid_count} ({uid_success_rate:.1f}% success rate)")
                print(f"   üö´ Anonymous users filtered: {anonymous_filtered}")
                print(f"   {mode_emoji} Mode: {mode_text}")
                print(f"   üì± Layout used: {layout}")
                print(f"   üíæ Saved to: {file_out}")
                
            else:
                self.lbl_status.config(text="‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y comment v·ªõi Enhanced UID logic", fg="#ffc107")
                self.lbl_progress_detail.config(text=f"üí° Layout: {layout} | Ki·ªÉm tra debug files ƒë·ªÉ ph√¢n t√≠ch Facebook structure")
                
                print(f"‚ö†Ô∏è No comments found with Enhanced UID logic")
                print(f"   üì± Layout: {layout}")
                print(f"   üîç Debug files created: debug_focused_{layout}.html")
                print(f"   üí° Suggestions:")
                print(f"      1. Check if you have access to the Facebook Group")
                print(f"      2. Verify the post URL is correct and public in the group")
                print(f"      3. Try running without headless mode to see what's happening")
                print(f"      4. Check the debug HTML file to understand the page structure")
                
        except Exception as e:
            error_msg = str(e)[:120]
            self.lbl_status.config(text=f"‚ùå L·ªói Enhanced UID scraping: {error_msg}...", fg="#dc3545")
            self.lbl_progress_detail.config(text="üîç Xem console ƒë·ªÉ bi·∫øt chi ti·∫øt. Enhanced UID version cung c·∫•p debug info.")
            print(f"Enhanced UID Groups scraping error: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.progress_bar.stop()
            if self.scraper: 
                self.scraper.close()
            self.btn_start.config(state=tk.NORMAL)
            self.btn_stop.config(state=tk.DISABLED)

# ----------------------------
# Run Enhanced UID app
# ----------------------------

if __name__ == "__main__":
    root = tk.Tk()
    app = FBGroupsAppGUI(root)
    root.mainloop()