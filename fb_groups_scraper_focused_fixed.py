# fb_groups_scraper_focused_fixed.py - FIXED: HTML div tags and scroll logic

import time, random, threading, re, requests, pandas as pd
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ----------------------------
# Helper utils (unchanged)
# ----------------------------

def parse_cookies_to_list(cookie_str):
    cookies_list = []
    for pair in cookie_str.split(';'):
        pair = pair.strip()
        if '=' in pair:
            name, value = pair.split('=', 1)
            cookies_list.append({'name': name.strip(), 'value': value.strip(), 'domain': '.facebook.com'})
    return cookies_list

def parse_cookies_to_dict(cookie_str):
    d = {}
    for pair in cookie_str.split(';'):
        pair = pair.strip()
        if '=' in pair:
            name, value = pair.split('=', 1)
            d[name.strip()] = value.strip()
    return d

def clean_text(text):
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text.strip())
    ui_patterns = [
        r'\b(Like|Reply|Share|Comment|Translate|Hide|Report|Block)\b',
        r'\b(Th√≠ch|Tr·∫£ l·ªùi|Chia s·∫ª|B√¨nh lu·∫≠n|D·ªãch|·∫®n|B√°o c√°o|Ch·∫∑n)\b',
        r'\b\d+\s*(min|minutes?|hours?|days?|seconds?|ph√∫t|gi·ªù|ng√†y|gi√¢y)\s*(ago|tr∆∞·ªõc)?\b',
        r'\b(Top fan|Most relevant|Newest|All comments|B√¨nh lu·∫≠n h√†ng ƒë·∫ßu)\b'
    ]
    for pattern in ui_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    return text.strip()

# ----------------------------
# FIXED Facebook Groups Scraper
# ----------------------------

class FacebookGroupsScraper:
    def __init__(self, cookie_str, headless=True):
        options = Options()
        if headless:
            options.add_argument("--headless=new")
        options.add_argument("--disable-notifications")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")

        # Better user agent for modern Facebook
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
        options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        self.wait = WebDriverWait(self.driver, 15)
        self.cookie_str = cookie_str or ""
        self.cookies_list = parse_cookies_to_list(self.cookie_str)
        self.cookies_dict = parse_cookies_to_dict(self.cookie_str)
        self._stop_flag = False
        self.current_layout = None
        
        if self.cookies_list:
            self._login_with_cookies()

    def _login_with_cookies(self):
        # Start with regular Facebook for better groups access
        self.driver.get("https://www.facebook.com")
        time.sleep(3)
        
        for c in self.cookies_list:
            cookie = c.copy()
            cookie.pop('sameSite', None)
            cookie.pop('httpOnly', None) 
            cookie.pop('secure', None)
            cookie.setdefault('domain', '.facebook.com')
            try:
                self.driver.add_cookie(cookie)
            except: 
                pass
        
        self.driver.get("https://www.facebook.com")
        time.sleep(4)

    def load_post(self, post_url):
        print(f"Loading groups post: {post_url}")
        
        urls_to_try = []
        
        if "groups/" in post_url:
            # Try www first for groups, then mobile, then mbasic
            www_url = post_url.replace("mbasic.facebook.com", "www.facebook.com").replace("m.facebook.com", "www.facebook.com")
            mobile_url = post_url.replace("www.facebook.com", "m.facebook.com").replace("mbasic.facebook.com", "m.facebook.com")
            mbasic_url = post_url.replace("www.facebook.com", "mbasic.facebook.com").replace("m.facebook.com", "mbasic.facebook.com")
            
            urls_to_try = [www_url, mobile_url, mbasic_url]
        else:
            urls_to_try = [post_url]
        
        for url_attempt in urls_to_try:
            try:
                print(f"Trying URL: {url_attempt}")
                self.driver.get(url_attempt)
                time.sleep(6)
                
                current_url = self.driver.current_url
                page_title = self.driver.title
                
                print(f"Current URL: {current_url}")
                print(f"Page title: {page_title}")
                
                # Detect layout
                if "m.facebook.com" in current_url:
                    self.current_layout = "mobile"
                elif "mbasic.facebook.com" in current_url:
                    self.current_layout = "mbasic"
                else:
                    self.current_layout = "www"
                
                print(f"Detected layout: {self.current_layout}")
                
                # Check login status
                if any(keyword in page_title.lower() for keyword in ["log in", "login", "ƒëƒÉng nh·∫≠p"]):
                    print("‚ùå Not logged in with this URL, trying next...")
                    continue
                
                print(f"‚úÖ Successfully loaded groups post with {self.current_layout} layout")
                
                # Try to switch to "All comments" view
                self._switch_to_all_comments()
                
                return True
                    
            except Exception as e:
                print(f"Failed to load {url_attempt}: {e}")
                continue
        
        print("‚ùå Failed to load post with any URL variant")
        return False

    def find_target_container(self):
        """FIXED: Find the target container with improved height detection"""
        print("üéØ Finding target container with larger height...")
        
        try:
            # Look for containers with data-thumb='1'
            containers = self.driver.find_elements(By.XPATH, "//div[@data-thumb='1']")
            
            if not containers:
                print("‚ö†Ô∏è No containers with data-thumb='1' found")
                return None
            
            print(f"Found {len(containers)} potential containers with data-thumb='1'")
            
            target_container = None
            max_height = 0
            
            for idx, container in enumerate(containers, 1):
                try:
                    # Get computed height
                    height = self.driver.execute_script("return arguments[0].offsetHeight;", container)
                    
                    # Also check style height
                    style = container.get_attribute('style') or ""
                    style_height_match = re.search(r'height:\s*(\d+)px', style)
                    style_height = int(style_height_match.group(1)) if style_height_match else 0
                    
                    # Use the larger of the two heights
                    actual_height = max(height, style_height)
                    
                    if actual_height > 0:
                        print(f" Container {idx}: height = {actual_height}px (offset: {height}px, style: {style_height}px)")
                    
                    if actual_height > max_height:
                        max_height = actual_height
                        target_container = container
                        
                except Exception as e:
                    print(f" Container {idx}: error getting height - {e}")
                    continue
            
            if target_container and max_height > 100:  # Minimum reasonable height
                print(f"‚úÖ Selected container with height: {max_height}px")
                
                # Scroll to make the container visible
                self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'start'});", target_container)
                time.sleep(2)
                
                return target_container
            else:
                print("‚ùå No suitable container found with sufficient height")
                return None
                
        except Exception as e:
            print(f"‚ùå Error finding target container: {e}")
            return None

    def scroll_to_load_all_comments(self):
        """FIXED: Enhanced scroll that ensures reaching the absolute bottom"""
        print("üìú Starting FIXED gradual scroll to load all comments...")
        
        try:
            # Find the comments container
            target_container = self.find_target_container()
            
            if not target_container:
                print("‚ùå Comments container not found")
                return False
            
            # Get container dimensions
            container_rect = target_container.rect
            container_top = container_rect['y']
            
            # Get the actual scrollable height
            container_scroll_height = self.driver.execute_script("return arguments[0].scrollHeight;", target_container)
            container_client_height = self.driver.execute_script("return arguments[0].clientHeight;", target_container)
            
            print(f"üìê Container dimensions:")
            print(f"   Top position: {container_top}px")
            print(f"   Scroll height: {container_scroll_height}px")
            print(f"   Client height: {container_client_height}px")
            
            # FIXED: Enhanced scroll strategy with absolute bottom guarantee
            scroll_step = 500  # Optimized steps for better loading
            scroll_pause = 2.5  # Longer pause for content loading
            max_attempts = 150  # Increased max attempts
            force_bottom_attempts = 5  # Number of times to force scroll to bottom
            
            current_scroll_position = 0
            no_change_count = 0
            previous_scroll_height = 0
            stuck_count = 0
            
            print("üöÄ Starting FIXED enhanced scroll sequence with absolute bottom guarantee...")
            
            # Start from the top of container
            self.driver.execute_script("arguments[0].scrollTop = 0;", target_container)
            time.sleep(1)
            
            for attempt in range(max_attempts):
                if self._stop_flag:
                    break
                
                # Get current scroll position
                current_scroll_position = self.driver.execute_script("return arguments[0].scrollTop;", target_container)
                current_scroll_height = self.driver.execute_script("return arguments[0].scrollHeight;", target_container)
                
                print(f"üìú Attempt {attempt+1}: scroll_pos={current_scroll_position}px, scroll_height={current_scroll_height}px")
                
                # FIXED: Check if we've reached the absolute bottom
                max_scroll_position = current_scroll_height - container_client_height
                
                if current_scroll_position >= max_scroll_position - 10:  # 10px tolerance
                    print("üèÅ FIXED: Reached absolute bottom of container!")
                    break
                
                # FIXED: Scroll to next position
                next_scroll_position = min(current_scroll_position + scroll_step, max_scroll_position)
                self.driver.execute_script(f"arguments[0].scrollTop = {next_scroll_position};", target_container)
                
                # Also scroll the main window to keep container in view
                window_scroll_y = container_top + (current_scroll_position * 0.5)  # Proportional scroll
                self.driver.execute_script(f"window.scrollTo(0, {window_scroll_y});")
                
                time.sleep(scroll_pause)
                
                # FIXED: Detect if content is still loading
                new_scroll_height = self.driver.execute_script("return arguments[0].scrollHeight;", target_container)
                
                if new_scroll_height > previous_scroll_height:
                    print(f"    ‚úÖ Content expanded: {previous_scroll_height}px -> {new_scroll_height}px")
                    no_change_count = 0
                    previous_scroll_height = new_scroll_height
                else:
                    no_change_count += 1
                    
                # FIXED: Enhanced stuck detection and recovery
                if no_change_count >= 3:
                    stuck_count += 1
                    print(f"    ‚ö° No content change detected (stuck #{stuck_count}), making big jump to bottom...")
                    
                    # FIXED: Multiple strategies to ensure we reach the absolute bottom
                    for bottom_attempt in range(force_bottom_attempts):
                        print(f"      üéØ Force bottom attempt {bottom_attempt + 1}/{force_bottom_attempts}")
                        
                        # Force scroll to absolute bottom
                        self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight;", target_container)
                        time.sleep(3)  # Longer wait for potential lazy loading
                        
                        # Also scroll main window to keep container visible
                        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)
                        
                        # Check if this revealed more content
                        final_scroll_height = self.driver.execute_script("return arguments[0].scrollHeight;", target_container)
                        
                        if final_scroll_height > new_scroll_height:
                            print(f"      üéØ Bottom jump {bottom_attempt + 1} revealed more content: {new_scroll_height}px -> {final_scroll_height}px")
                            previous_scroll_height = final_scroll_height
                            no_change_count = 0
                            stuck_count = 0
                            break
                        elif bottom_attempt == force_bottom_attempts - 1:
                            print(f"      üèÅ All {force_bottom_attempts} bottom attempts completed - confirmed at absolute bottom")
                            # Final verification
                            current_scroll_top = self.driver.execute_script("return arguments[0].scrollTop;", target_container)
                            max_scroll_top = self.driver.execute_script("return arguments[0].scrollHeight - arguments[0].clientHeight;", target_container)
                            print(f"      üìä Final position: {current_scroll_top}px / {max_scroll_top}px")
                            return True
                    
                    # If we're still stuck after multiple attempts, break
                    if stuck_count >= 3:
                        print("    üõë FIXED: Multiple stuck attempts completed, assuming all content loaded")
                        break
            
            # FIXED: Final verification - ensure we're truly at the bottom
            print("\nüîç FIXED: Final verification of scroll position...")
            
            final_scroll_top = self.driver.execute_script("return arguments[0].scrollTop;", target_container)
            final_scroll_height = self.driver.execute_script("return arguments[0].scrollHeight;", target_container)
            final_client_height = self.driver.execute_script("return arguments[0].clientHeight;", target_container)
            
            max_possible_scroll = final_scroll_height - final_client_height
            
            print(f"üìä Final metrics:")
            print(f"   Current scroll position: {final_scroll_top}px")
            print(f"   Max possible scroll: {max_possible_scroll}px")
            print(f"   Total scroll height: {final_scroll_height}px")
            
            if final_scroll_top < max_possible_scroll - 20:  # 20px tolerance
                print("‚ö° FIXED: Force scrolling to absolute bottom...")
                self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight;", target_container)
                time.sleep(3)
                
                # Final check
                truly_final_scroll_top = self.driver.execute_script("return arguments[0].scrollTop;", target_container)
                print(f"‚úÖ FIXED: Final scroll position: {truly_final_scroll_top}px")
            
            print("üéØ FIXED scroll sequence completed - ensured absolute bottom reached!")
            return True
            
        except Exception as e:
            print(f"‚ùå Error during FIXED comment scrolling: {e}")
            return False

    def complete_scroll_sequence(self):
        """FIXED: Complete 2-phase scrolling with enhanced bottom detection"""
        print("üé¨ Starting FIXED complete scroll sequence...")
        
        # Phase 1: Find and scroll to target container
        print("\n=== PHASE 1: Finding target container ===")
        target_container = self.find_target_container()
        
        if not target_container:
            print("‚ùå Phase 1 failed - no target container found")
            return False
        
        print("‚úÖ Phase 1 completed - target container found and scrolled to")
        time.sleep(2)
        
        # Phase 2: FIXED gradual scroll through comments
        print("\n=== PHASE 2: FIXED - Loading all comments to absolute bottom ===")
        success = self.scroll_to_load_all_comments()
        
        if success:
            print("üéâ FIXED complete scroll sequence finished successfully!")
        else:
            print("‚ö†Ô∏è Comment loading phase had issues, but container was found")
        
        return success

    def _switch_to_all_comments(self):
        """FIXED: Switch to 'All comments' view with proper HTML parsing"""
        print("üîÑ Attempting to switch to 'All comments' view...")
        
        try:
            time.sleep(3)
            
            # FIXED: Enhanced selectors for all comments button with proper HTML structure
            all_comments_selectors = [
                # FIXED: Properly formed XPath selectors for Vietnamese
                "//span[contains(normalize-space(text()),'T·∫•t c·∫£ b√¨nh lu·∫≠n')]",
                "//div[contains(normalize-space(text()),'T·∫•t c·∫£ b√¨nh lu·∫≠n')]",
                "//a[contains(normalize-space(text()),'T·∫•t c·∫£ b√¨nh lu·∫≠n')]",
                "//button[contains(normalize-space(text()),'T·∫•t c·∫£ b√¨nh lu·∫≠n')]",
                
                # FIXED: English selectors
                "//span[contains(normalize-space(text()),'All comments')]",
                "//div[contains(normalize-space(text()),'All comments')]",
                "//a[contains(normalize-space(text()),'All comments')]",
                "//button[contains(normalize-space(text()),'All comments')]",
                
                # FIXED: Role-based selectors with proper text matching
                "//div[@role='button' and (contains(normalize-space(text()),'T·∫•t c·∫£ b√¨nh lu·∫≠n') or contains(normalize-space(text()),'All comments'))]",
                "//span[@role='button' and (contains(normalize-space(text()),'T·∫•t c·∫£ b√¨nh lu·∫≠n') or contains(normalize-space(text()),'All comments'))]",
                
                # FIXED: Aria-label selectors
                "//div[contains(@aria-label,'comment') and contains(normalize-space(text()),'All')]",
                "//div[contains(@aria-label,'b√¨nh lu·∫≠n') and contains(normalize-space(text()),'T·∫•t c·∫£')]",
                
                # FIXED: Additional fallback selectors
                "//*[contains(normalize-space(text()),'T·∫•t c·∫£ b√¨nh lu·∫≠n') or contains(normalize-space(text()),'All comments')]"
            ]
            
            clicked = False
            for selector in all_comments_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    for element in elements:
                        if element.is_displayed() and element.is_enabled():
                            element_text = element.text.strip()
                            print(f"  Found 'All comments' button: {element_text}")
                            
                            # FIXED: Better validation of the button text
                            if ('t·∫•t c·∫£ b√¨nh lu·∫≠n' in element_text.lower() or 
                                'all comments' in element_text.lower()):
                                
                                # Scroll into view
                                self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", element)
                                time.sleep(1)
                                
                                # Try to click
                                try:
                                    element.click()
                                    clicked = True
                                    print("  ‚úÖ Successfully clicked 'All comments' button")
                                    time.sleep(4)  # Wait for comments to load
                                    break
                                except:
                                    # Try JavaScript click
                                    try:
                                        self.driver.execute_script("arguments[0].click();", element)
                                        clicked = True
                                        print("  ‚úÖ Successfully clicked 'All comments' button (JS)")
                                        time.sleep(4)
                                        break
                                    except:
                                        continue
                    
                    if clicked:
                        break
                        
                except Exception as e:
                    continue
            
            if not clicked:
                print("  ‚ö†Ô∏è Could not find or click 'All comments' button, proceeding with current view")
            else:
                print("  üéØ Switched to 'All comments' view successfully")
                
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error switching to 'All comments' view: {e}")
            print("  Proceeding with current view...")

    def extract_groups_comments(self):
        """FIXED: Comment extraction targeting larger height container"""
        print(f"=== EXTRACTING GROUPS COMMENTS (FIXED) ===")
        
        # FIXED: Find and focus on the target container
        print("üéØ Finding and scrolling to target container...")
        target_container = self.find_target_container()
        
        if not target_container:
            print("‚ùå Could not find target container")
            return []
        
        # FIXED: Ensure we have all content loaded by scrolling to bottom first
        print("üéØ FIXED: Ensuring all content is loaded...")
        self.complete_scroll_sequence()
        
        # Re-find container after scrolling (it might have changed)
        target_container = self.find_target_container()
        
        # Save page for debugging
        try:
            with open(f"debug_fixed_{self.current_layout}.html", "w", encoding="utf-8") as f:
                f.write(self.driver.page_source)
            print(f"Saved page to debug_fixed_{self.current_layout}.html")
        except:
            pass
        
        # FIXED: Search within the target container first
        all_comment_elements = []
        
        print("üéØ FIXED: Searching within target container...")
        
        # Strategy 1: Layout-specific selectors within target container
        if self.current_layout == "www":
            selectors = [
                ".//div[@role='article']",
                ".//div[contains(@aria-label, 'Comment by')]",
                ".//div[contains(@aria-label, 'B√¨nh lu·∫≠n c·ªßa')]",
                ".//div[.//a[contains(@href, '/user/') or contains(@href, '/profile/')]]",
                ".//div[.//h3//a[contains(@href, 'facebook.com')]]",
                # FIXED: Additional selectors for better coverage
                ".//div[.//a[contains(@href, 'facebook.com')] and string-length(normalize-space(text())) > 20]",
                ".//div[contains(@class, 'comment') or contains(@data-testid, 'comment')]"
            ]
        elif self.current_layout == "mobile":
            selectors = [
                ".//div[@data-sigil='comment']",
                ".//div[contains(@data-ft, 'comment')]",
                ".//div[contains(@id, 'comment_')]",
                ".//div[.//a[contains(@href, 'profile.php') or contains(@href, 'user.php')]]",
                # FIXED: Additional mobile selectors
                ".//div[.//a[contains(@href, 'facebook.com')] and string-length(normalize-space(text())) > 15]"
            ]
        else:  # mbasic
            selectors = [
                ".//div[@data-ft and contains(@data-ft, 'comment')]",
                ".//div[contains(@id, 'comment_')]",
                ".//table//div[.//a[contains(@href, 'profile.php')]]",
                ".//div[.//a[contains(@href, 'profile.php?id=')]]",
                # FIXED: Additional mbasic selectors
                ".//div[.//a[contains(@href, 'facebook.com')] and string-length(normalize-space(text())) > 10]"
            ]
        
        # Apply selectors within target container
        for i, selector in enumerate(selectors):
            try:
                elements = target_container.find_elements(By.XPATH, selector)
                print(f"Target container - Selector {i+1}: Found {len(elements)} elements")
                
                for elem in elements:
                    if elem not in all_comment_elements:
                        all_comment_elements.append(elem)
                        
            except Exception as e:
                print(f"Target container - Selector {i+1} failed: {e}")
                continue
        
        # Strategy 2: If not enough elements, expand search
        if len(all_comment_elements) < 10:
            print("‚ö†Ô∏è Not enough elements in target container, expanding search...")
            
            # Search in the entire page as fallback
            for i, selector in enumerate(selectors):
                try:
                    # Remove leading "./" to search entire document
                    global_selector = selector.replace(".//", "//")
                    elements = self.driver.find_elements(By.XPATH, global_selector)
                    print(f"Global search - Selector {i+1}: Found {len(elements)} elements")
                    
                    for elem in elements:
                        if elem not in all_comment_elements:
                            all_comment_elements.append(elem)
                            
                except Exception as e:
                    print(f"Global search - Selector {i+1} failed: {e}")
                    continue
        
        # Strategy 3: FIXED fallback selectors with better filtering
        if len(all_comment_elements) == 0:
            print("‚ö†Ô∏è No comments with standard selectors, trying FIXED fallback...")
            
            fallback_selectors = [
                # FIXED: Look for any div with profile links and meaningful text
                "//div[.//a[contains(@href, 'facebook.com/')] and string-length(normalize-space(text())) > 20]",
                "//div[string-length(normalize-space(text())) > 30 and .//a[@href]]",
                "//div[@role='article' and string-length(normalize-space(text())) > 20]",
                "//*[.//a[contains(@href, 'profile')] and string-length(normalize-space(text())) > 15]",
                # FIXED: Additional patterns
                "//div[contains(@class, 'x') and .//a[contains(@href, 'facebook.com')] and string-length(normalize-space(text())) > 25]"
            ]
            
            for selector in fallback_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    print(f"FIXED fallback selector: Found {len(elements)} elements")
                    for elem in elements:
                        if elem not in all_comment_elements:
                            all_comment_elements.append(elem)
                    
                    # Stop if we found enough elements
                    if len(all_comment_elements) > 20:
                        break
                except:
                    continue
        
        # Sort by position
        try:
            all_comment_elements.sort(key=lambda x: (x.location['y'], x.location['x']))
        except:
            pass
        
        comments = []
        seen_content = set()
        
        print(f"FIXED: Processing {len(all_comment_elements)} potential comment elements...")
        
        # Process each element
        for i, element in enumerate(all_comment_elements):
            if self._stop_flag:
                break
                
            try:
                print(f"\n--- FIXED Element {i+1}/{len(all_comment_elements)} ---")
                
                comment_data = self.extract_comment_data_fixed(element, i)
                
                if not comment_data:
                    continue
                
                # Deduplication
                if comment_data['Name'] == "Unknown":
                    print("  ‚úó Skipped: no username found")
                    continue
                    
                # Check for duplicates
                content_signature = f"{comment_data['Name']}_{comment_data['ProfileLink']}"
                if content_signature in seen_content:
                    print("  ‚úó Skipped: duplicate user")
                    continue
                seen_content.add(content_signature)
                
                comment_data['Type'] = 'Comment'
                comment_data['Layout'] = self.current_layout
                comment_data['Source'] = 'Target Container (FIXED)'
                
                comments.append(comment_data)
                print(f"  ‚úÖ Added: {comment_data['Name']} - Profile: {comment_data['ProfileLink'][:50]}...")
                
            except Exception as e:
                print(f"  Error processing element {i}: {e}")
                continue
        
        print(f"\n=== FIXED EXTRACTION COMPLETE: {len(comments)} comments ===")
        return comments

    def extract_comment_data_fixed(self, element, index):
        """FIXED: Enhanced comment data extraction with better link analysis"""
        try:
            full_text = element.text.strip()
            if len(full_text) < 5:
                print(f"  ‚ùå Text too short: '{full_text}'")
                return None
            
            print(f"  FIXED Processing: '{full_text[:60]}...'")
            
            # Skip anonymous users
            if any(keyword in full_text.lower() for keyword in ['·∫©n danh', 'ng∆∞·ªùi tham gia ·∫©n danh', 'anonymous']):
                print("  ‚ö†Ô∏è Skipping anonymous user comment")
                return None
            
            username = "Unknown"
            profile_href = ""
            uid = "Unknown"
            
            # FIXED: Enhanced username extraction with better validation
            print(f"    üéØ FIXED analysis of element structure...")
            
            try:
                all_links = element.find_elements(By.XPATH, ".//a")
                print(f"    Found {len(all_links)} total links in element")
                
                # FIXED: Prioritize links that are more likely to be usernames
                potential_profile_links = []
                
                for link_index, link in enumerate(all_links):
                    try:
                        link_text = link.text.strip()
                        link_href = link.get_attribute("href") or ""
                        
                        print(f"      Link {link_index+1}: Text='{link_text}' | Href={link_href[:60]}...")
                        
                        # FIXED: Enhanced Facebook profile link detection
                        is_facebook_profile = ('facebook.com' in link_href and 
                                             ('profile.php' in link_href or 
                                              '/user/' in link_href or 
                                              'user.php' in link_href or
                                              re.search(r'facebook\.com/[^/]+/?$', link_href)))  # Direct profile URLs
                        
                        if is_facebook_profile:
                            # FIXED: Enhanced name validation
                            is_valid_name = (link_text and 
                                           len(link_text) >= 2 and 
                                           len(link_text) <= 100 and
                                           not link_text.isdigit() and
                                           not link_text.startswith('http') and
                                           not re.match(r'^\d+$', link_text) and  # Not just numbers
                                           not any(ui in link_text.lower() for ui in [
                                               'like', 'reply', 'share', 'comment', 'th√≠ch', 'tr·∫£ l·ªùi', 
                                               'chia s·∫ª', 'b√¨nh lu·∫≠n', 'ago', 'tr∆∞·ªõc', 'min', 'hour', 
                                               'day', 'ph√∫t', 'gi·ªù', 'ng√†y', '·∫©n danh', 'anonymous',
                                               'view', 'xem', 'show', 'hi·ªÉn th·ªã', 'see more', 'view more',
                                               'translate', 'd·ªãch', 'hide', '·∫©n', 'report', 'b√°o c√°o'
                                           ]))
                            
                            if is_valid_name:
                                # FIXED: Calculate priority score for this link
                                priority_score = 0
                                
                                # Higher priority for shorter, cleaner names
                                if len(link_text) < 50:
                                    priority_score += 10
                                
                                # Higher priority for names with proper capitalization
                                if link_text[0].isupper():
                                    priority_score += 5
                                
                                # Higher priority for names with spaces (likely full names)
                                if ' ' in link_text:
                                    priority_score += 3
                                
                                # Extract UID
                                extracted_uid = "Unknown"
                                uid_patterns = [
                                    r'profile\.php\?id=(\d+)',
                                    r'user\.php\?id=(\d+)',
                                    r'/user/(\d+)',
                                    r'id=(\d+)',
                                    r'facebook\.com/([^/?]+)',  # Username from URL
                                    r'(\d{10,})'  # Facebook UIDs are usually 10+ digits
                                ]
                                
                                for pattern in uid_patterns:
                                    uid_match = re.search(pattern, link_href)
                                    if uid_match:
                                        extracted_uid = uid_match.group(1)
                                        priority_score += 5  # Bonus for having UID
                                        break
                                
                                potential_profile_links.append({
                                    'text': link_text,
                                    'href': link_href,
                                    'uid': extracted_uid,
                                    'priority': priority_score,
                                    'index': link_index
                                })
                                
                                print(f"      ‚úÖ FIXED: Valid profile candidate: {link_text} (priority: {priority_score}) -> UID: {extracted_uid}")
                                
                    except Exception as e:
                        print(f"      Error processing link {link_index+1}: {e}")
                        continue
                
                # FIXED: Select the best profile link based on priority
                if potential_profile_links:
                    # Sort by priority (highest first)
                    potential_profile_links.sort(key=lambda x: x['priority'], reverse=True)
                    
                    best_link = potential_profile_links[0]
                    username = best_link['text']
                    profile_href = best_link['href']
                    uid = best_link['uid']
                    
                    print(f"      üéØ FIXED: Selected best profile: {username} (priority: {best_link['priority']}) -> UID: {uid}")
                
            except Exception as e:
                print(f"    Error in FIXED method: {e}")
            
            # Final validation
            if username == "Unknown":
                print("  ‚ùå FIXED extraction failed for this element")
                return None
                
            print(f"  ‚úÖ FIXED: Successfully extracted username: {username}")
            
            return {
                "UID": uid,
                "Name": username,
                "ProfileLink": profile_href,
                "CommentLink": "",
                "ElementIndex": index,
                "TextPreview": full_text[:100] + "..." if len(full_text) > 100 else full_text,
                "ContainerHeight": "FIXED - Enhanced scroll to absolute bottom"
            }
            
        except Exception as e:
            print(f"Error in FIXED extraction: {e}")
            return None

    def expand_groups_comments(self, max_iterations=50):
        """FIXED: Enhanced expansion with better bottom detection"""
        print(f"=== EXPANDING GROUPS COMMENTS (FIXED) ===")
        
        # Find target container initially
        target_container = self.find_target_container()
        
        for iteration in range(max_iterations):
            if self._stop_flag:
                break
                
            print(f"[Iteration {iteration+1}] FIXED scrolling and expanding...")
            
            # Re-find target container every 5 iterations or if previous one is stale
            if iteration % 5 == 0 or not target_container:
                target_container = self.find_target_container()
            
            # FIXED: Enhanced container scrolling
            if target_container:
                try:
                    # Get current scroll state
                    current_scroll_top = self.driver.execute_script("return arguments[0].scrollTop;", target_container)
                    scroll_height = self.driver.execute_script("return arguments[0].scrollHeight;", target_container)
                    client_height = self.driver.execute_script("return arguments[0].clientHeight;", target_container)
                    
                    # FIXED: Calculate how much more we can scroll
                    max_scroll = scroll_height - client_height
                    remaining_scroll = max_scroll - current_scroll_top
                    
                    print(f"    Container scroll state: {current_scroll_top}/{max_scroll}px (remaining: {remaining_scroll}px)")
                    
                    if remaining_scroll > 10:  # Still more to scroll
                        # Scroll down by a reasonable amount
                        new_scroll_position = min(current_scroll_top + 800, max_scroll)
                        self.driver.execute_script(f"arguments[0].scrollTop = {new_scroll_position};", target_container)
                    else:
                        # FIXED: We're at the bottom, force scroll to absolute bottom
                        self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight;", target_container)
                        print("    üéØ FIXED: Forced scroll to absolute bottom of container")
                        
                    time.sleep(1)
                except Exception as e:
                    print(f"    Container scroll failed, re-finding: {e}")
                    target_container = self.find_target_container()
            
            # Also scroll the main page
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(2, 3))
            
            # FIXED: Look for expand links with enhanced detection
            expand_selectors = [
                "//a[contains(normalize-space(text()),'View more comments')]",
                "//a[contains(normalize-space(text()),'Xem th√™m b√¨nh lu·∫≠n')]",
                "//a[contains(normalize-space(text()),'Show more')]",
                "//a[contains(normalize-space(text()),'See more')]",
                "//div[@role='button' and (contains(normalize-space(text()),'more') or contains(normalize-space(text()),'th√™m'))]",
                # FIXED: Additional expand selectors
                "//span[contains(normalize-space(text()),'View more') or contains(normalize-space(text()),'Xem th√™m')]",
                "//*[@role='button' and (contains(normalize-space(text()),'more comments') or contains(normalize-space(text()),'th√™m b√¨nh lu·∫≠n'))]"
            ]
            
            expanded = False
            for selector in expand_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    for elem in elements:
                        if elem.is_displayed() and elem.is_enabled():
                            try:
                                # FIXED: Scroll element into view before clicking
                                self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", elem)
                                time.sleep(1)
                                
                                elem.click()
                                expanded = True
                                print(f"    ‚úì FIXED: Clicked: {elem.text}")
                                time.sleep(3)
                                break
                            except:
                                try:
                                    self.driver.execute_script("arguments[0].click();", elem)
                                    expanded = True
                                    print(f"    ‚úì FIXED: JS clicked: {elem.text}")
                                    time.sleep(3)
                                    break
                                except:
                                    continue
                    if expanded:
                        break
                except:
                    continue
            
            if not expanded and iteration > 5:
                print(f"    No expansion found, stopping early")
                break
        
        print("=== FIXED EXPANSION COMPLETE ===")

    def scrape_all_comments(self, limit=0, resolve_uid=True, progress_callback=None):
        """FIXED: Main scraping orchestrator with enhanced scroll logic"""
        print(f"=== STARTING FIXED GROUPS SCRAPING ===")
        
        # Step 1: Expand all content with FIXED logic
        self.expand_groups_comments()
        
        if self._stop_flag:
            return []
        
        # Step 2: Extract comments with FIXED logic
        comments = self.extract_groups_comments()
        
        # Step 3: Apply limit
        if limit > 0:
            comments = comments[:limit]
        
        # Step 4: Progress reporting
        if progress_callback:
            progress_callback(len(comments))
        
        return comments

    def close(self):
        try: 
            self.driver.quit()
        except: 
            pass

# ----------------------------
# FIXED GUI
# ----------------------------

class FBGroupsAppGUI:
    def __init__(self, root):
        self.root = root
        root.title("üéØ FB Groups Comment Scraper - FIXED")
        root.geometry("1100x950")
        root.configure(bg="#e8f5e8")

        # Main frame
        main_frame = tk.Frame(root, bg="#e8f5e8")
        main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=15)

        # Header
        header_frame = tk.Frame(main_frame, bg="#e8f5e8")
        header_frame.pack(fill="x", pady=(0,20))
        
        title_label = tk.Label(header_frame, text="üéØ Facebook Groups Comment Scraper - FIXED", 
                              font=("Arial", 20, "bold"), bg="#e8f5e8", fg="#2d5a2d")
        title_label.pack()
        
        # FIXED: Proper HTML structure in subtitle
        subtitle_text = "üéØ FIXED version - Enhanced scroll to absolute bottom + Fixed HTML parsing"
        subtitle_label = tk.Label(header_frame, text=subtitle_text, 
                                 font=("Arial", 11), bg="#e8f5e8", fg="#5a5a5a")
        subtitle_label.pack(pady=(5,0))

        # Input section
        input_frame = tk.LabelFrame(main_frame, text="üìù Th√¥ng tin b√†i vi·∫øt Groups", font=("Arial", 12, "bold"), 
                                   bg="#e8f5e8", fg="#2d5a2d", relief="groove", bd=2)
        input_frame.pack(fill="x", pady=(0,15))

        tk.Label(input_frame, text="üîó Link b√†i vi·∫øt trong Groups:", bg="#e8f5e8", font=("Arial", 10)).pack(anchor="w", padx=15, pady=(15,5))
        self.entry_url = tk.Entry(input_frame, width=100, font=("Arial", 9))
        self.entry_url.pack(fill="x", padx=15, pady=(0,10))

        tk.Label(input_frame, text="üç™ Cookie Facebook (ƒë·ªÉ truy c·∫≠p Groups):", bg="#e8f5e8", font=("Arial", 10)).pack(anchor="w", padx=15, pady=(5,5))
        self.txt_cookie = tk.Text(input_frame, height=4, font=("Arial", 8))
        self.txt_cookie.pack(fill="x", padx=15, pady=(0,15))

        # Options section
        options_frame = tk.LabelFrame(main_frame, text="üéØ C·∫•u h√¨nh FIXED Version", font=("Arial", 12, "bold"), 
                                     bg="#e8f5e8", fg="#2d5a2d", relief="groove", bd=2)
        options_frame.pack(fill="x", pady=(0,15))
        
        opt_grid = tk.Frame(options_frame, bg="#e8f5e8")
        opt_grid.pack(fill="x", padx=15, pady=15)
        
        # Options grid
        tk.Label(opt_grid, text="üìä S·ªë l∆∞·ª£ng comment:", bg="#e8f5e8").grid(row=0, column=0, sticky="w")
        self.entry_limit = tk.Entry(opt_grid, width=10)
        self.entry_limit.insert(0, "0")
        self.entry_limit.grid(row=0, column=1, sticky="w", padx=(10,20))
        tk.Label(opt_grid, text="(0 = t·∫•t c·∫£)", bg="#e8f5e8", fg="#6c757d").grid(row=0, column=2, sticky="w")

        self.headless_var = tk.BooleanVar(value=False)  # Default to visible for debugging
        tk.Checkbutton(opt_grid, text="üëª Ch·∫°y ·∫©n", variable=self.headless_var,
                      bg="#e8f5e8", font=("Arial", 9)).grid(row=1, column=0, sticky="w", pady=(10,0))

        self.resolve_uid_var = tk.BooleanVar(value=True)
        tk.Checkbutton(opt_grid, text="üÜî L·∫•y UID", variable=self.resolve_uid_var, 
                      bg="#e8f5e8", font=("Arial", 9)).grid(row=1, column=1, sticky="w", pady=(10,0))

        # File section
        file_frame = tk.LabelFrame(main_frame, text="üíæ Xu·∫•t k·∫øt qu·∫£", font=("Arial", 12, "bold"), 
                                  bg="#e8f5e8", fg="#2d5a2d", relief="groove", bd=2)
        file_frame.pack(fill="x", pady=(0,15))
        
        file_row = tk.Frame(file_frame, bg="#e8f5e8")
        file_row.pack(fill="x", padx=15, pady=15)
        
        self.entry_file = tk.Entry(file_row, width=70, font=("Arial", 9))
        self.entry_file.insert(0, "facebook_groups_comments_FIXED.xlsx")
        self.entry_file.pack(side="left", fill="x", expand=True)
        
        tk.Button(file_row, text="üìÅ Ch·ªçn", command=self.choose_file, 
                 bg="#17a2b8", fg="white", font=("Arial", 9)).pack(side="right", padx=(10,0))

        # Status section
        status_frame = tk.LabelFrame(main_frame, text="üìä Tr·∫°ng th√°i th·ª±c thi - FIXED", font=("Arial", 12, "bold"), 
                                    bg="#e8f5e8", fg="#2d5a2d", relief="groove", bd=2)
        status_frame.pack(fill="x", pady=(0,15))
        
        self.lbl_status = tk.Label(status_frame, text="‚úÖ FIXED scraper s·∫µn s√†ng - ƒê√£ fix scroll logic v√† HTML parsing", fg="#28a745", 
                                  wraplength=900, justify="left", font=("Arial", 11), bg="#e8f5e8")
        self.lbl_status.pack(anchor="w", padx=15, pady=(15,5))

        # FIXED: Properly formatted status detail
        fixed_features_text = ("üí° FIXED features: 1) Enhanced scroll to absolute bottom, "
                              "2) Better link priority scoring, 3) Improved HTML structure parsing, "
                              "4) Force scroll verification, 5) Enhanced debugging output")
        
        self.lbl_progress_detail = tk.Label(status_frame, text=fixed_features_text,
                                          fg="#6c757d", wraplength=900, justify="left", font=("Arial", 9), bg="#e8f5e8")
        self.lbl_progress_detail.pack(anchor="w", padx=15, pady=(0,10))

        # Progress bar
        self.progress_bar = ttk.Progressbar(status_frame, mode='indeterminate')
        self.progress_bar.pack(fill="x", padx=15, pady=(0,15))

        # Control buttons
        button_frame = tk.Frame(main_frame, bg="#e8f5e8")
        button_frame.pack(fill="x", pady=20)
        
        self.btn_start = tk.Button(button_frame, text="üöÄ B·∫Øt ƒë·∫ßu FIXED Scraping", bg="#28a745", fg="white", 
                                  font=("Arial", 14, "bold"), command=self.start_scrape_thread, 
                                  pady=12, padx=40)
        self.btn_start.pack(side="left")

        self.btn_stop = tk.Button(button_frame, text="‚èπÔ∏è D·ª´ng", bg="#dc3545", fg="white", 
                                 font=("Arial", 14, "bold"), command=self.stop_scrape, 
                                 state=tk.DISABLED, pady=12, padx=40)
        self.btn_stop.pack(side="left", padx=(25,0))

        self.progress_var = tk.IntVar(value=0)
        self.progress_label = tk.Label(button_frame, textvariable=self.progress_var, fg="#28a745", 
                                     font=("Arial", 18, "bold"), bg="#e8f5e8")
        self.progress_label.pack(side="right")

        self._scrape_thread = None
        self._stop_flag = False
        self.scraper = None

    def choose_file(self):
        f = filedialog.asksaveasfilename(
            defaultextension=".xlsx", 
            filetypes=[("Excel files", "*.xlsx"), ("CSV files", "*.csv")],
            title="Ch·ªçn file ƒë·ªÉ l∆∞u FIXED Groups comments"
        )
        if f:
            self.entry_file.delete(0, tk.END)
            self.entry_file.insert(0, f)

    def start_scrape_thread(self):
        url = self.entry_url.get().strip()
        cookie_str = self.txt_cookie.get("1.0", tk.END).strip()
        file_out = self.entry_file.get().strip() or "facebook_groups_comments_FIXED.xlsx"
        
        if not url:
            messagebox.showerror("‚ùå L·ªói", "Vui l√≤ng nh·∫≠p link b√†i vi·∫øt Groups.")
            return
        
        if "groups/" not in url:
            result = messagebox.askyesno("‚ö†Ô∏è X√°c nh·∫≠n", 
                                       "Link n√†y c√≥ v·∫ª kh√¥ng ph·∫£i Groups. B·∫°n c√≥ mu·ªën ti·∫øp t·ª•c kh√¥ng?")
            if not result:
                return
        
        try: 
            limit = int(self.entry_limit.get().strip())
        except: 
            limit = 0

        self._stop_flag = False
        self.progress_var.set(0)
        self.progress_bar.start()
        self.lbl_status.config(text="üîÑ ƒêang kh·ªüi ƒë·ªông FIXED Groups scraper...", fg="#fd7e14")
        self.lbl_progress_detail.config(text="‚è≥ Initializing FIXED extraction logic with enhanced scroll and HTML parsing...")
        self.btn_start.config(state=tk.DISABLED)
        self.btn_stop.config(state=tk.NORMAL)

        self._scrape_thread = threading.Thread(target=self._scrape_worker, 
                                             args=(url, cookie_str, file_out, limit, 
                                                   self.headless_var.get(), self.resolve_uid_var.get()))
        self._scrape_thread.daemon = True
        self._scrape_thread.start()

    def stop_scrape(self):
        self._stop_flag = True
        if self.scraper:
            self.scraper._stop_flag = True
        self.lbl_status.config(text="‚èπÔ∏è ƒêang d·ª´ng FIXED scraper...", fg="#dc3545")
        self.btn_stop.config(state=tk.DISABLED)

    def _progress_cb(self, count):
        self.progress_var.set(count)
        self.lbl_status.config(text=f"üìà FIXED processing... ƒê√£ l·∫•y {count} comments", fg="#28a745")
        self.root.update_idletasks()

    def _scrape_worker(self, url, cookie_str, file_out, limit, headless, resolve_uid):
        try:
            # Initialize
            self.lbl_status.config(text="üåê Kh·ªüi t·∫°o FIXED Groups scraper...", fg="#fd7e14")
            self.scraper = FacebookGroupsScraper(cookie_str, headless=headless)
            
            if self._stop_flag: return
            
            # Load post
            self.lbl_status.config(text="üìÑ ƒêang t·∫£i b√†i vi·∫øt Groups v·ªõi FIXED logic...", fg="#fd7e14")
            self.lbl_progress_detail.config(text="‚è≥ Loading post with enhanced error handling...")
            success = self.scraper.load_post(url)
            
            if not success:
                self.lbl_status.config(text="‚ùå Kh√¥ng th·ªÉ t·∫£i b√†i vi·∫øt Groups", fg="#dc3545")
                self.lbl_progress_detail.config(text="üí° Ki·ªÉm tra: 1) Cookie valid, 2) Quy·ªÅn truy c·∫≠p Groups, 3) Link ch√≠nh x√°c")
                return
            
            # Show detected layout
            layout = getattr(self.scraper, 'current_layout', 'unknown')
            self.lbl_progress_detail.config(text=f"üéØ Layout detected: {layout} - Using FIXED extraction methods...")
                
            if self._stop_flag: return
            
            # Scrape with FIXED logic
            self.lbl_status.config(text=f"üîç FIXED Groups extraction ({layout})...", fg="#fd7e14")
            self.lbl_progress_detail.config(text="‚è≥ Using enhanced scroll logic and username extraction...")
            
            comments = self.scraper.scrape_all_comments(limit=limit, resolve_uid=resolve_uid, 
                                                       progress_callback=self._progress_cb)
            
            if self._stop_flag: return
            
            # Save
            self.lbl_status.config(text="üíæ ƒêang l∆∞u FIXED Groups data...", fg="#fd7e14")
            
            if comments:
                df = pd.DataFrame(comments)
                
                # Add metadata
                df.insert(0, 'STT', range(1, len(df) + 1))
                df['Source'] = 'Facebook Groups - FIXED'
                df['ScrapedAt'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # File handling
                if not file_out.lower().endswith((".xlsx", ".csv")):
                    file_out += ".xlsx"
                
                if file_out.lower().endswith(".csv"):
                    df.to_csv(file_out, index=False, encoding="utf-8-sig")
                else:
                    df.to_excel(file_out, index=False, engine="openpyxl")
                
                # Statistics
                unique_users = len(set(c['Name'] for c in comments if c['Name'] != 'Unknown'))
                profile_links = len([c for c in comments if c['ProfileLink']])
                uid_count = len([c for c in comments if c['UID'] != 'Unknown'])
                
                self.lbl_status.config(text=f"üéâ FIXED GROUPS SCRAPING HO√ÄN TH√ÄNH!", fg="#28a745")
                self.lbl_progress_detail.config(text=f"üìä FIXED Results: {len(comments)} comments | {unique_users} unique users | {profile_links} profile links | {uid_count} UIDs | Layout: {layout} | File: {file_out}")
                
                print(f"üéØ FIXED SCRAPING COMPLETE!")
                print(f"   üìä Results: {len(comments)} total comments")
                print(f"   üë• Unique users: {unique_users}")
                print(f"   üîó Profile links: {profile_links}")
                print(f"   üÜî UIDs extracted: {uid_count}")
                print(f"   üì± Layout used: {layout}")
                print(f"   üíæ Saved to: {file_out}")
                print(f"   üîç Debug files: debug_fixed_{layout}.html")
                
            else:
                self.lbl_status.config(text="‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y comment v·ªõi FIXED logic", fg="#ffc107")
                self.lbl_progress_detail.config(text=f"üí° Layout: {layout} | Ki·ªÉm tra debug files ƒë·ªÉ ph√¢n t√≠ch Facebook structure")
                
                print(f"‚ö†Ô∏è No comments found with FIXED logic")
                print(f"   üì± Layout: {layout}")
                print(f"   üîç Debug files created: debug_fixed_{layout}.html")
                print(f"   üí° FIXED Suggestions:")
                print(f"      1. Check if you have access to the Facebook Group")
                print(f"      2. Verify the post URL is correct and public in the group")
                print(f"      3. Try running without headless mode to see what's happening")
                print(f"      4. Check the debug HTML file to understand the page structure")
                print(f"      5. FIXED scroll logic should now reach absolute bottom")
                
        except Exception as e:
            error_msg = str(e)[:120]
            self.lbl_status.config(text=f"‚ùå L·ªói FIXED scraping: {error_msg}...", fg="#dc3545")
            self.lbl_progress_detail.config(text="üîç Xem console ƒë·ªÉ bi·∫øt chi ti·∫øt. FIXED version cung c·∫•p nhi·ªÅu debug info.")
            print(f"FIXED Groups scraping error: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.progress_bar.stop()
            if self.scraper: 
                self.scraper.close()
            self.btn_start.config(state=tk.NORMAL)
            self.btn_stop.config(state=tk.DISABLED)

# ----------------------------
# Run FIXED app
# ----------------------------

if __name__ == "__main__":
    root = tk.Tk()
    app = FBGroupsAppGUI(root)
    root.mainloop()